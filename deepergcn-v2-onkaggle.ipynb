{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# %% [code]\n# ------------------ install torch_geometric begin -----------------\ntry:\n    import torch_geometric\nexcept:\n    import subprocess\n    import torch\n\n    nvcc_stdout = str(subprocess.check_output(['nvcc', '-V']))\n    tmp = nvcc_stdout[nvcc_stdout.rfind('release') + len('release') + 1:]\n    cuda_version = tmp[:tmp.find(',')]\n    cuda = {\n            '9.2': 'cu92',\n            '10.1': 'cu101',\n            '10.2': 'cu102',\n            }\n\n    CUDA = cuda[cuda_version]\n    TORCH = torch.__version__.split('.')\n    TORCH[-1] = '0'\n    TORCH = '.'.join(TORCH)\n\n    install1 = 'pip install torch-scatter==latest+' + CUDA + ' -f https://pytorch-geometric.com/whl/torch-' + TORCH + '.html'\n    install2 = 'pip install torch-sparse==latest+' + CUDA + ' -f https://pytorch-geometric.com/whl/torch-' + TORCH + '.html'\n    install3 = 'pip install torch-cluster==latest+' + CUDA + ' -f https://pytorch-geometric.com/whl/torch-' + TORCH + '.html'\n    install4 = 'pip install torch-spline-conv==latest+' + CUDA + ' -f https://pytorch-geometric.com/whl/torch-' + TORCH + '.html'\n    install5 = 'pip install torch-geometric'\n\n    subprocess.run(install1.split())\n    subprocess.run(install2.split())\n    subprocess.run(install3.split())\n    subprocess.run(install4.split())\n    subprocess.run(install5.split())\n# ------------------ install torch_geometric end -----------------","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport torch\nfrom torch.nn import Linear, LayerNorm, ReLU, Dropout\nfrom torch_geometric.nn import ChebConv, NNConv, DeepGCNLayer, ARMAConv\nfrom torch_geometric.data import Data, DataLoader\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nfrom sklearn.cluster import KMeans\n\nfrom tqdm import tqdm\nimport os\nimport copy\n#import gc\nfrom IPython.core.debugger import set_trace\n\n# settings\nseed = 777\ntrain_file = '../input/stanford-covid-vaccine/train.json'\ntest_file = '../input/stanford-covid-vaccine/test.json'\nbpps_top = '../input/stanford-covid-vaccine/bpps'\nnb_fold = 5\ndevice = 'cuda'\nbatch_size = 16\nepochs = 130 #100\nlr = 0.001\nnlabel = 5\ntrain_with_noisy_data = True\nadd_edge_for_paired_nodes = True\nadd_codon_nodes = True\nT = 5 #15\nnode_hidden_channels = 128 # 96 #128\nedge_hidden_channels = 16\nhidden_channels3 = 32\nnum_layers = 10\ndropout1 = 0.1\ndropout2 = 0.1\ndropout3 = 0.1\nbpps_nb_mean = 0.077522 # mean of bpps_nb across all training data\nbpps_nb_std = 0.08914   # std of bpps_nb across all training data\nerror_mean_limit = 0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def match_pair(structure):\n    pair = [-1] * len(structure)\n    pair_no = -1\n\n    pair_no_stack = []\n    for i, c in enumerate(structure):\n        if c == '(':\n            pair_no += 1\n            pair[i] = pair_no\n            pair_no_stack.append(pair_no)\n        elif c == ')':\n            pair[i] = pair_no_stack.pop()\n    return pair\n\nclass MyData(Data):\n    def __init__(self, x=None, edge_index=None, edge_attr=None, y=None,\n                 pos=None, norm=None, face=None, weight=None, **kwargs):\n        super(MyData, self).__init__(x=x, edge_index=edge_index,\n                                     edge_attr=edge_attr, y=y, pos=pos,\n                                     norm=norm, face=face, **kwargs)\n        self.weight = weight\n        \ndef calc_error_mean(row):\n    reactivity_error = row['reactivity_error']\n    deg_error_Mg_pH10 = row['deg_error_Mg_pH10']\n    deg_error_Mg_50C = row['deg_error_Mg_50C']\n\n    return np.mean(np.abs(reactivity_error) +\n                   np.abs(deg_error_Mg_pH10) + \\\n                   np.abs(deg_error_Mg_50C)) / 3\n\ndef calc_sample_weight(row):\n    if sample_is_clean(row):\n        return 1.\n    else:\n        error_mean = calc_error_mean(row)\n        if error_mean >= error_mean_limit:\n            return 0.\n\n        return 1. - error_mean / error_mean_limit\n    \n# def calc_sample_weight(row):\n#     return np.log(row.signal_to_noise+1.1)/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add directed edge for node1 -> node2 and for node2 -> node1\ndef add_edges(edge_index, edge_features, node1, node2, feature1, feature2):\n    edge_index.append([node1, node2])\n    edge_features.append(feature1)\n    edge_index.append([node2, node1])\n    edge_features.append(feature2)\n\ndef add_edges_between_base_nodes(edge_index, edge_features, node1, node2):\n    edge_feature1 = [\n        0, # is edge for paired nodes\n        0, # is edge between codon node and base node\n        0, # is edge between coden nodes\n        1, # forward edge: 1, backward edge: -1\n        1, # bpps if edge is for paired nodes\n    ]\n    edge_feature2 = [\n        0, # is edge for paired nodes\n        0, # is edge between codon node and base node\n        0, # is edge between coden nodes\n        -1, # forward edge: 1, backward edge: -1\n        1, # bpps if edge is for paired nodes\n    ]\n    add_edges(edge_index, edge_features, node1, node2,\n              edge_feature1, edge_feature2)\n\ndef add_edges_between_paired_nodes(edge_index, edge_features, node1, node2,\n                                   bpps_value):\n    edge_feature1 = [\n        1, # is edge for paired nodes\n        0, # is edge between codon node and base node\n        0, # is edge between coden nodes\n        0, # forward edge: 1, backward edge: -1\n        bpps_value, # bpps if edge is for paired nodes\n    ]\n    edge_feature2 = [\n        1, # is edge for paired nodes\n        0, # is edge between codon node and base node\n        0, # is edge between coden nodes\n        0, # forward edge: 1, backward edge: -1\n        bpps_value, # bpps if edge is for paired nodes\n    ]\n    add_edges(edge_index, edge_features, node1, node2,\n              edge_feature1, edge_feature2)\n\ndef add_edges_between_codon_nodes(edge_index, edge_features, node1, node2):\n    edge_feature1 = [\n        0, # is edge for paired nodes\n        0, # is edge between codon node and base node\n        1, # is edge between coden nodes\n        1, # forward edge: 1, backward edge: -1\n        0, # bpps if edge is for paired nodes\n    ]\n    edge_feature2 = [\n        0, # is edge for paired nodes\n        0, # is edge between codon node and base node\n        1, # is edge between coden nodes\n        -1, # forward edge: 1, backward edge: -1\n        0, # bpps if edge is for paired nodes\n    ]\n    add_edges(edge_index, edge_features, node1, node2,\n              edge_feature1, edge_feature2)\n\ndef add_edges_between_codon_and_base_node(edge_index, edge_features,\n                                          node1, node2):\n    edge_feature1 = [\n        0, # is edge for paired nodes\n        1, # is edge between codon node and base node\n        0, # is edge between coden nodes\n        0, # forward edge: 1, backward edge: -1\n        0, # bpps if edge is for paired nodes\n    ]\n    edge_feature2 = [\n        0, # is edge for paired nodes\n        1, # is edge between codon node and base node\n        0, # is edge between coden nodes\n        0, # forward edge: 1, backward edge: -1\n        0, # bpps if edge is for paired nodes\n    ]\n    add_edges(edge_index, edge_features, node1, node2,\n              edge_feature1, edge_feature2)\n\ndef add_node(node_features, feature):\n    node_features.append(feature)\n\ndef add_base_node(node_features, sequence, predicted_loop_type,\n                  bpps_sum, bpps_nb):\n    feature = [\n        0, # is codon node\n        sequence == 'A',\n        sequence == 'C',\n        sequence == 'G',\n        sequence == 'U',\n        predicted_loop_type == 'S',\n        predicted_loop_type == 'M',\n        predicted_loop_type == 'I',\n        predicted_loop_type == 'B',\n        predicted_loop_type == 'H',\n        predicted_loop_type == 'E',\n        predicted_loop_type == 'X',\n        bpps_sum,\n        bpps_nb,\n    ]\n    add_node(node_features, feature)\n\ndef add_codon_node(node_features):\n    feature = [\n        1, # is codon node\n        0, # sequence == 'A',\n        0, # sequence == 'C',\n        0, # sequence == 'G',\n        0, # sequence == 'U',\n        0, # predicted_loop_type == 'S',\n        0, # predicted_loop_type == 'M',\n        0, # predicted_loop_type == 'I',\n        0, # predicted_loop_type == 'B',\n        0, # predicted_loop_type == 'H',\n        0, # predicted_loop_type == 'E',\n        0, # predicted_loop_type == 'X',\n        0, # bpps_sum\n        0, # bpps_nb\n    ]\n    add_node(node_features, feature)\n\ndef build_data(df, is_train):\n    data = []\n    for i in range(len(df)):\n        targets = []\n        node_features = []\n        edge_features = []\n        edge_index = []\n        train_mask = []\n        test_mask = []\n        weights = []\n\n        id = df.loc[i, 'id']\n        path = os.path.join(bpps_top, id + '.npy')\n        bpps = np.load(path)\n        bpps_sum = bpps.sum(axis=0)\n        sequence = df.loc[i, 'sequence']\n        structure = df.loc[i, 'structure']\n        pair_info = match_pair(structure)\n        predicted_loop_type = df.loc[i, 'predicted_loop_type']\n        seq_length = df.loc[i, 'seq_length']\n        seq_scored = df.loc[i, 'seq_scored']\n        bpps_nb = (bpps > 0).sum(axis=0) / seq_length\n        bpps_nb = (bpps_nb - bpps_nb_mean) / bpps_nb_std\n        if is_train:\n            sample_weight = calc_sample_weight(df.loc[i])\n\n            reactivity = df.loc[i, 'reactivity']\n            deg_Mg_pH10 = df.loc[i, 'deg_Mg_pH10']\n            deg_pH10 = df.loc[i, 'deg_pH10']\n            deg_Mg_50C = df.loc[i, 'deg_Mg_50C']\n            deg_50C = df.loc[i, 'deg_50C']\n\n            for j in range(seq_length):\n                if j < seq_scored:\n                    targets.append([\n                        reactivity[j],\n                        deg_Mg_pH10[j],\n                        deg_pH10[j],\n                        deg_Mg_50C[j],\n                        deg_50C[j],\n                        ])\n                else:\n                    targets.append([0, 0, 0, 0, 0])\n\n        paired_nodes = {}\n        for j in range(seq_length):\n            add_base_node(node_features, sequence[j], predicted_loop_type[j],\n                          bpps_sum[j], bpps_nb[j])\n\n            if j + 1 < seq_length: # edge between current node and next node\n                add_edges_between_base_nodes(edge_index, edge_features,\n                                             j, j + 1)\n\n            if pair_info[j] != -1:\n                if pair_info[j] not in paired_nodes:\n                    paired_nodes[pair_info[j]] = [j]\n                else:\n                    paired_nodes[pair_info[j]].append(j)\n\n            train_mask.append(j < seq_scored)\n            test_mask.append(True)\n            if is_train:\n                weights.append(sample_weight)\n\n        if add_edge_for_paired_nodes:\n            for pair in paired_nodes.values():\n                bpps_value = bpps[pair[0], pair[1]]\n                add_edges_between_paired_nodes(edge_index, edge_features,\n                                               pair[0], pair[1], bpps_value)\n\n        if add_codon_nodes:\n            codon_node_idx = seq_length - 1\n            for j in range(seq_length):\n                if j % 5 == 0: \n                    # add codon node\n                    add_codon_node(node_features)\n                    codon_node_idx += 1\n                    train_mask.append(False)\n                    test_mask.append(False)\n                    if is_train:\n                        weights.append(0)\n                        targets.append([0, 0, 0, 0, 0])\n\n                    if codon_node_idx > seq_length:\n                        # add edges between adjacent codon nodes\n                        add_edges_between_codon_nodes(edge_index, edge_features,\n                                                      codon_node_idx - 1,\n                                                      codon_node_idx)\n\n                # add edges between codon node and base node\n                add_edges_between_codon_and_base_node(edge_index, edge_features,\n                                                      j, codon_node_idx)\n\n        node_features = torch.tensor(node_features, dtype=torch.float)\n        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n        edge_features = torch.tensor(edge_features, dtype=torch.float)\n\n        if is_train:\n            data.append(MyData(x=node_features, edge_index=edge_index,\n                               edge_attr=edge_features,\n                               train_mask=torch.tensor(train_mask),\n                               weight=torch.tensor(weights, dtype=torch.float),\n                               y=torch.tensor(targets, dtype=torch.float)))\n        else:\n            data.append(MyData(x=node_features, edge_index=edge_index,\n                               edge_attr=edge_features,\n                               test_mask=torch.tensor(test_mask)))\n\n    return data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MapE2NxN(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, hidden_channels):\n        super(MapE2NxN, self).__init__()\n        self.linear1 = Linear(in_channels, hidden_channels)\n        self.linear2 = Linear(hidden_channels, out_channels)\n        self.dropout = Dropout(dropout3)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.dropout(x)\n        x = self.linear2(x)\n        return x\n\nclass MyDeeperGCN(torch.nn.Module):\n    def __init__(self, num_node_features, num_edge_features,\n                 node_hidden_channels,\n                 edge_hidden_channels,\n                 num_layers, num_classes):\n        super(MyDeeperGCN, self).__init__()\n\n        #self.node_encoder = ChebConv(num_node_features, node_hidden_channels, T)\n        self.node_encoder = ARMAConv(num_node_features, node_hidden_channels, num_stacks = 4, \n                                    num_layers = 8, dropout = 0.1) # improve CV && LB\n        self.edge_encoder = Linear(num_edge_features, edge_hidden_channels)\n\n        self.layers = torch.nn.ModuleList()\n        for i in range(1, num_layers + 1):\n            conv = NNConv(node_hidden_channels, node_hidden_channels,\n                          MapE2NxN(edge_hidden_channels,\n                                   node_hidden_channels * node_hidden_channels,\n                                   hidden_channels3))\n            norm = LayerNorm(node_hidden_channels, elementwise_affine=True)\n            act = ReLU(inplace=True)\n\n            layer = DeepGCNLayer(conv, norm, act, block='res+',\n                                 dropout=dropout1, ckpt_grad=i % 3)\n            self.layers.append(layer)\n\n        self.lin = Linear(node_hidden_channels, num_classes)\n        self.dropout = Dropout(dropout2)\n\n    def forward(self, data):\n        x = data.x\n        edge_index = data.edge_index\n        edge_attr = data.edge_attr\n\n        # edge for paired nodes are excluded for encoding node\n        seq_edge_index = edge_index[:, edge_attr[:,0] == 0]\n        x = self.node_encoder(x, seq_edge_index)\n\n        edge_attr = self.edge_encoder(edge_attr)\n\n        x = self.layers[0].conv(x, edge_index, edge_attr)\n\n        for layer in self.layers[1:]:\n            x = layer(x, edge_index, edge_attr)\n\n        x = self.layers[0].act(self.layers[0].norm(x))\n        x = self.dropout(x)\n\n        return self.lin(x)\n\ndef weighted_mse_loss(prds, tgts, weight):\n    return torch.mean(weight * (prds - tgts)**2)\n\ndef criterion(prds, tgts, weight=None):\n    if weight is None:\n        return (torch.sqrt(torch.nn.MSELoss()(prds[:,0], tgts[:,0])) +\n                torch.sqrt(torch.nn.MSELoss()(prds[:,1], tgts[:,1])) +\n                torch.sqrt(torch.nn.MSELoss()(prds[:,2], tgts[:,2])) + \n                torch.sqrt(torch.nn.MSELoss()(prds[:,3], tgts[:,3])) + \n                torch.sqrt(torch.nn.MSELoss()(prds[:,4], tgts[:,4]))) / nlabel\n    else:\n        return (torch.sqrt(weighted_mse_loss(prds[:,0], tgts[:,0], weight)) +\n                torch.sqrt(weighted_mse_loss(prds[:,1], tgts[:,1], weight)) +\n                torch.sqrt(weighted_mse_loss(prds[:,2], tgts[:,2], weight)) +\n               torch.sqrt(weighted_mse_loss(prds[:,3], tgts[:,3], weight)) +\n               torch.sqrt(weighted_mse_loss(prds[:,4], tgts[:,4], weight))) / nlabel\n\n# def criterion(y_actual, y_pred, weight=None, num_scored=3):\n#     score = 0\n#     for i in range(num_scored):\n#         score += torch.sqrt(((y_actual[:,i]-y_pred[:,i])**2).mean(0)) / num_scored\n#     if weight is not None:\n#         score *= weight\n#     return score.mean(0)\n    \ndef build_id_seqpos(df):\n    id_seqpos = []\n    for i in range(len(df)):\n        id = df.loc[i, 'id']\n        seq_length = df.loc[i, 'seq_length']\n        for seqpos in range(seq_length):\n            id_seqpos.append(id + '_' + str(seqpos))\n    return id_seqpos\n\ndef build_id_seqpos_68(df):\n    id_seqpos = []\n    for i in range(len(df)):\n        id = df.loc[i, 'id']\n        seq_length = 68\n        for seqpos in range(seq_length):\n            id_seqpos.append(id + '_' + str(seqpos))\n    return id_seqpos\n\ndef sample_is_clean(row):\n    return row['SN_filter'] == 1\n    #return row['signal_to_noise'] > 1 and \\\n    #       min((min(row['reactivity']),\n    #            min(row['deg_Mg_pH10']),\n    #            min(row['deg_pH10']),\n    #            min(row['deg_Mg_50C']),\n    #            min(row['deg_50C']))) > -0.5\n\n# categorical value for target (used for stratified kfold)\ndef add_y_cat(df):\n    target_mean = df['reactivity'].apply(np.mean) + \\\n                  df['deg_Mg_pH10'].apply(np.mean) + \\\n                  df['deg_pH10'].apply(np.mean) + \\\n                  df['deg_Mg_50C'].apply(np.mean) + \\\n                  df['deg_50C'].apply(np.mean)\n    df['y_cat'] = pd.qcut(np.array(target_mean), q=20).codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    # not enough ? reproducibility issue still remains\n    \nseed_everything(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug_df = pd.read_csv('../input/openvaccine-augdata/aug_data1.csv')\n\ndef aug_data(df):\n    target_df = df.copy()\n    new_df = aug_df[aug_df['id'].isin(target_df['id'])]\n                         \n    del target_df['structure']\n    del target_df['predicted_loop_type']\n    new_df = new_df.merge(target_df, on=['id','sequence'], how='left')\n\n    df['cnt'] = df['id'].map(new_df[['id','cnt']].set_index('id').to_dict()['cnt'])\n    df['log_gamma'] = 100\n    df['score'] = 1.0\n    df = df.append(new_df[df.columns])\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\ndef preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    base_fea = np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )\n    bpps_sum_fea = np.array(df['bpps_sum'].to_list())[:,:,np.newaxis]\n    bpps_max_fea = np.array(df['bpps_max'].to_list())[:,:,np.newaxis]\n    bpps_nb_fea = np.array(df['bpps_nb'].to_list())[:,:,np.newaxis]\n    return np.concatenate([base_fea,bpps_sum_fea,bpps_max_fea,bpps_nb_fea], 2)\n\n# additional features\n\ndef read_bpps_sum(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"../input/stanford-covid-vaccine/bpps/{mol_id}.npy\").sum(axis=1))\n    return bpps_arr\n\ndef read_bpps_max(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"../input/stanford-covid-vaccine/bpps/{mol_id}.npy\").max(axis=1))\n    return bpps_arr\n\ndef read_bpps_nb(df):\n    # from https://www.kaggle.com/tuckerarrants/openvaccine-gru-lstm \n    bpps_nb_mean = 0.077522 # mean of bpps_nb across all training data\n    bpps_nb_std = 0.08914   # std of bpps_nb across all training data\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps = np.load(f\"../input/stanford-covid-vaccine/bpps/{mol_id}.npy\")\n        bpps_nb = (bpps > 0).sum(axis=0) / bpps.shape[0]\n        bpps_nb = (bpps_nb - bpps_nb_mean) / bpps_nb_std\n        bpps_arr.append(bpps_nb)\n    return bpps_arr ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Reading', train_file)\ndf_tr = pd.read_json(train_file, lines=True)\n\n#df_tr['bpps_sum'] = read_bpps_sum(df_tr)\n#df_tr['bpps_max'] = read_bpps_max(df_tr)\n#df_tr['bpps_nb'] = read_bpps_nb(df_tr)\n\n# clustering for  GroupKFold\n#kmeans_model = KMeans(n_clusters=200, random_state=seed).fit(preprocess_inputs(df_tr)[:,:,0])\n#df_tr['cluster_id'] = kmeans_model.labels_\n\n\ndf_tr = aug_data(df_tr)\n\nadd_y_cat(df_tr)\n\nis_clean = df_tr.apply(sample_is_clean, axis=1)\ndf_clean = df_tr[is_clean].reset_index(drop=True)\ndf_noisy = df_tr[is_clean==False].reset_index(drop=True)\ndel df_tr\n\nprint('Training')\nall_ys = torch.zeros((0, nlabel)).to(device).detach()\nall_outs = torch.zeros((0, nlabel)).to(device).detach()\nall_ids = []\nbest_model_states = []\n# kf = StratifiedKFold(nb_fold, shuffle=True, random_state=seed)\nkf = GroupKFold(nb_fold)\nfor fold, ((clean_train_idx, clean_valid_idx),\n           (noisy_train_idx, noisy_valid_idx)) \\\n               in enumerate(zip(kf.split(df_clean, df_clean['reactivity'], df_clean['id']), # df_clean['reactivity'], groups = df_clean['cluster_id']\n                                kf.split(df_noisy, df_noisy['reactivity'], df_noisy['id']))): # trying split with reactivity instead of y_cat\n    print('Fold', fold)\n\n    # build train data\n    df_train = df_clean.loc[clean_train_idx]\n    if train_with_noisy_data:\n        df_train_noisy = df_noisy.loc[noisy_train_idx]\n        df_train_noisy = \\\n           df_train_noisy[df_train_noisy.apply(calc_error_mean, axis=1) <= \\\n                          error_mean_limit]\n        df_train = df_train.append(df_train_noisy)\n    data_train = build_data(df_train.reset_index(drop=True), True)\n    del df_train\n    loader_train = DataLoader(data_train, batch_size=batch_size,\n                              shuffle=True)\n\n    # build validation data\n    df_valid_clean = df_clean.loc[clean_valid_idx].reset_index(drop=True)\n    data_valid_clean = build_data(df_valid_clean, True)\n    all_ids.extend(build_id_seqpos_68(df_valid_clean))\n    del df_valid_clean\n    loader_valid_clean = DataLoader(data_valid_clean, batch_size=batch_size,\n                                    shuffle=False)\n\n    model = MyDeeperGCN(data_train[0].num_node_features,\n                        data_train[0].num_edge_features,\n                        node_hidden_channels=node_hidden_channels,\n                        edge_hidden_channels=edge_hidden_channels,\n                        num_layers=num_layers,\n                        num_classes=nlabel).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    best_mcrmse = np.inf\n    for epoch in range(epochs):\n        print('Epoch', epoch)\n        model.train()\n        train_loss = 0.0\n        nb = 0\n        for data in tqdm(loader_train):\n            data = data.to(device)\n            mask = data.train_mask\n            weight = data.weight[mask]\n\n            optimizer.zero_grad()\n            out = model(data)[mask]\n            y = data.y[mask]\n            loss = criterion(out, y, weight)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * y.size(0)\n            nb += y.size(0)\n\n            del data\n            del out\n            del y\n            del loss\n            #gc.collect()\n            #torch.cuda.empty_cache()\n        train_loss /= nb\n\n        model.eval()\n        valid_loss = 0.0\n        nb = 0\n        ys = torch.zeros((0, nlabel)).to(device).detach()\n        outs = torch.zeros((0, nlabel)).to(device).detach()\n        for data in tqdm(loader_valid_clean):\n            data = data.to(device)\n            mask = data.train_mask\n\n            out = model(data)[mask].detach()\n            y = data.y[mask].detach()\n            loss = criterion(out, y).detach()\n            valid_loss += loss.item() * y.size(0)\n            nb += y.size(0)\n\n            outs = torch.cat((outs, out), dim=0)\n            ys = torch.cat((ys, y), dim=0)\n\n            del data\n            del out\n            del y\n            del loss\n            #gc.collect()\n            #torch.cuda.empty_cache()\n        valid_loss /= nb\n\n        mcrmse = criterion(outs, ys).item()\n\n        print(\"Train Loss: {:.4f} ---- Valid Loss: {:.4f} ---- Valid MCRMSE: {:.4f}\".\\\n                format(train_loss, valid_loss, mcrmse))\n\n        if mcrmse < best_mcrmse:\n            print('Best valid MCRMSE updated to', mcrmse)\n            best_mcrmse = mcrmse\n            best_model_state = copy.deepcopy(model.state_dict())\n\n    del data_train\n    del data_valid_clean\n    #gc.collect()\n    #torch.cuda.empty_cache()\n\n    best_model_states.append(best_model_state)\n\n    # predict for CV\n    model.load_state_dict(best_model_state)\n    model.eval()\n    for data in tqdm(loader_valid_clean):\n        data = data.to(device)\n        mask = data.train_mask\n\n        out = model(data)[mask].detach()\n        y = data.y[mask].detach()\n\n        all_ys = torch.cat((all_ys, y), dim=0)\n        all_outs = torch.cat((all_outs, out), dim=0)\n\n        del data\n        del out\n        del y\n        #gc.collect()\n        #torch.cuda.empty_cache()\n\n# calculate MCRMSE by all training data\nprint('CV MCRMSE ', criterion(all_outs, all_ys).item())\n# del all_outs\n# del all_ys\n#gc.collect()\n#torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_outs = all_outs.cpu()\ndf_valid = pd.DataFrame({'id_seqpos': all_ids,\n                       'reactivity': all_outs[:,0],\n                       'deg_Mg_pH10': all_outs[:,1],\n                       'deg_pH10': all_outs[:,2],\n                       'deg_Mg_50C': all_outs[:,3],\n                       'deg_50C': all_outs[:,4]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_valid.to_csv('validation.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# predict for test data\nprint('Predicting test data')\nprint('Reading', test_file)\ndf_te = pd.read_json(test_file, lines=True)\n\n#df_te['bpps_sum'] = read_bpps_sum(df_te)\n#df_te['bpps_max'] = read_bpps_max(df_te)\n#df_te['bpps_nb'] = read_bpps_nb(df_te)\n\ndf_te = aug_data(df_te)\n\ndata_test = build_data(df_te.reset_index(drop=True), False)\nloader_test = DataLoader(data_test, batch_size=batch_size, shuffle=False)\nid_seqpos = build_id_seqpos(df_te.reset_index(drop=True))\n\npreds = torch.zeros((len(id_seqpos), nlabel)).to(device).detach()\nfor best_model_state in best_model_states:\n    model.load_state_dict(best_model_state)\n    model.eval()\n\n    outs = torch.zeros((0, nlabel)).to(device).detach()\n    for data in tqdm(loader_test):\n        data = data.to(device)\n        mask = data.test_mask\n\n        out = model(data)[mask].detach()\n        outs = torch.cat((outs, out), dim=0)\n\n        del data\n        del out\n        #gc.collect()\n        #torch.cuda.empty_cache()\n    preds += outs\npreds /= len(best_model_states)\npreds = preds.cpu().numpy()\n\ndf_sub = pd.DataFrame({'id_seqpos': id_seqpos,\n                       'reactivity': preds[:,0],\n                       'deg_Mg_pH10': preds[:,1],\n                       'deg_pH10': preds[:,2],\n                       'deg_Mg_50C': preds[:,3],\n                       'deg_50C': preds[:,4]})\nprint('Writing submission.csv')\ndf_sub.to_csv('submission_all.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_df = pd.read_csv('../input/stanford-covid-vaccine/sample_submission.csv')\n\ntarget_cols = [c for c in sample_df.columns if c != 'id_seqpos']\n\nlist_id = list(sample_df.id_seqpos.values)\noutput = {}\noutput_df = pd.DataFrame({'id_seqpos': sample_df.id_seqpos.values})\n\nfor c in target_cols:\n    output_values = []\n    x = df_sub.groupby('id_seqpos')[c].mean().reset_index()\n    print(x.shape)\n    output_df = pd.merge(output_df, x, on='id_seqpos')\n\noutput_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"output_df.to_csv('submission.csv', index = False)\noutput_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}