{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training scheme\n",
    "- (1) train denoising auto encoder model using all data including train and test data\n",
    "- (2) from the weights of denoising auto encoder model, finetune to predict targets such as reactivity\n",
    "\n",
    "### rough network architecture\n",
    "- inputs -> conv1ds -> aggregation of neighborhoods -> multi head attention -> aggregation of neighborhoods -> multi head attention -> conv1d -> predict\n",
    "- this architecture was inspired by https://www.kaggle.com/cpmpml/graph-transfomer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "pretrain_dir = None # model dir for resuming training. if None, train from scrach\n",
    "\n",
    "one_fold = False # if True, train model at only first fold. use if you try a new idea quickly.\n",
    "run_test = False # if True, use small data. you can check whether this code run or not\n",
    "denoise = True # if True, use train data whose signal_to_noise > 1\n",
    "\n",
    "ae_epochs = 20 # epoch of training of denoising auto encoder\n",
    "ae_epochs_each = 5 # epoch of training of denoising auto encoder each time. \n",
    "                   # I use train data (seqlen = 107) and private test data (seqlen = 130) for auto encoder training.\n",
    "                   # I dont know how to easily fit keras model to use both of different shape data simultaneously, \n",
    "                   # so I call fit function several times. \n",
    "ae_batch_size = 32\n",
    "\n",
    "# epochs_list = [30, 10, 3, 3, 5, 5]\n",
    "epochs_list = [30, 10, 5, 5, 8, 8]\n",
    "batch_size_list = [8, 16, 32, 64, 128, 256] \n",
    "\n",
    "## copy pretrain model to working dir\n",
    "import shutil\n",
    "import glob\n",
    "import ast\n",
    "if pretrain_dir is not None:\n",
    "    for d in glob.glob(pretrain_dir + \"*\"):\n",
    "        shutil.copy(d, \".\")\n",
    "    \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df = pd.read_csv('../OpenVaccine/aug_data1.csv')\n",
    "def aug_data(df):\n",
    "    target_df = df.copy()\n",
    "    new_df = aug_df[aug_df['id'].isin(target_df['id'])]\n",
    "                         \n",
    "    del target_df['structure']\n",
    "    del target_df['predicted_loop_type']\n",
    "    new_df = new_df.merge(target_df, on=['id','sequence'], how='left')\n",
    "\n",
    "#     df['cnt'] = df['id'].map(new_df[['id','cnt']].set_index('id').to_dict()['cnt'])\n",
    "#     df['log_gamma'] = 100\n",
    "#     df['score'] = 1.0\n",
    "    df = df.append(new_df[df.columns])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_df = pd.read_csv('../OpenVaccine/pseudo_test.csv')\n",
    "\n",
    "pseudo_df['reactivity'] = pseudo_df['reactivity'].apply(lambda x: ast.literal_eval(x))\n",
    "pseudo_df['deg_Mg_pH10'] = pseudo_df['deg_Mg_pH10'].apply(lambda x: ast.literal_eval(x))\n",
    "pseudo_df['deg_pH10'] = pseudo_df['deg_pH10'].apply(lambda x: ast.literal_eval(x))\n",
    "pseudo_df['deg_Mg_50C'] = pseudo_df['deg_Mg_50C'].apply(lambda x: ast.literal_eval(x))\n",
    "pseudo_df['deg_50C'] = pseudo_df['deg_50C'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_st = pseudo_df[pseudo_df['seq_length'] == 107]\n",
    "pseudo_lg = pseudo_df[pseudo_df['seq_length'] == 130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thinh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/thinh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/thinh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/thinh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/thinh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/home/thinh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/home/thinh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/thinh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/thinh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/thinh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "pseudo_st['reactivity'] = pseudo_st['reactivity'].apply(lambda x: x[:68])\n",
    "pseudo_st['deg_Mg_pH10'] = pseudo_st['deg_Mg_pH10'].apply(lambda x: x[:68])\n",
    "pseudo_st['deg_pH10'] = pseudo_st['deg_pH10'].apply(lambda x: x[:68])\n",
    "pseudo_st['deg_Mg_50C'] = pseudo_st['deg_Mg_50C'].apply(lambda x: x[:68])\n",
    "pseudo_st['deg_50C'] = pseudo_st['deg_50C'].apply(lambda x: x[:68])\n",
    "\n",
    "pseudo_lg['reactivity'] = pseudo_lg['reactivity'].apply(lambda x: x[:91])\n",
    "pseudo_lg['deg_Mg_pH10'] = pseudo_lg['deg_Mg_pH10'].apply(lambda x: x[:91])\n",
    "pseudo_lg['deg_pH10'] = pseudo_lg['deg_pH10'].apply(lambda x: x[:91])\n",
    "pseudo_lg['deg_Mg_50C'] = pseudo_lg['deg_Mg_50C'].apply(lambda x: x[:91])\n",
    "pseudo_lg['deg_50C'] = pseudo_lg['deg_50C'].apply(lambda x: x[:91])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>structure</th>\n",
       "      <th>predicted_loop_type</th>\n",
       "      <th>seq_length</th>\n",
       "      <th>seq_scored</th>\n",
       "      <th>reactivity</th>\n",
       "      <th>deg_Mg_pH10</th>\n",
       "      <th>deg_pH10</th>\n",
       "      <th>deg_Mg_50C</th>\n",
       "      <th>deg_50C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>id_000ae4237</td>\n",
       "      <td>GGAAACGGGUUCCGCGGAUUGCUGCUAAUAAGAGUAAUCUCUAAAU...</td>\n",
       "      <td>.....((((..((((((...(((((.....((((....)))).......</td>\n",
       "      <td>EEEEESSSSIISSSSSSIIISSSSSIIIIISSSSHHHHSSSSIIII...</td>\n",
       "      <td>130</td>\n",
       "      <td>91</td>\n",
       "      <td>[0.5988852160599594, 1.252383497027623, 1.1766...</td>\n",
       "      <td>[0.5802506685595211, 1.9965426237137536, 0.765...</td>\n",
       "      <td>[1.6855349656016847, 1.9856331898502928, 0.841...</td>\n",
       "      <td>[0.4643303444983725, 1.7353359778870612, 0.980...</td>\n",
       "      <td>[0.5961263638596626, 1.330144615692037, 0.9792...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>id_0020473f7</td>\n",
       "      <td>GGAAACCCGCCCGCGCCCGCCCGCGCUGCUGCCGUGCCUCCUCUCC...</td>\n",
       "      <td>.....(((((((((((((((((((((((((((((((((((((((((...</td>\n",
       "      <td>EEEEESSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS...</td>\n",
       "      <td>130</td>\n",
       "      <td>91</td>\n",
       "      <td>[0.751697894404419, 1.8868896799083001, 1.3745...</td>\n",
       "      <td>[0.7895877059930309, 2.8275117665075093, 0.935...</td>\n",
       "      <td>[1.467933834294549, 2.6756719629474697, 1.0947...</td>\n",
       "      <td>[0.5991996450286899, 2.477975121643563, 1.2246...</td>\n",
       "      <td>[0.5419860902101901, 1.5332587823832124, 1.105...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>id_002852873</td>\n",
       "      <td>GGAAAGCGAAACGCCGAGAAGACGUAGUUCGCAGAGCGGCGUACCC...</td>\n",
       "      <td>.....(((...(((......(((((((((.((....(((.....))...</td>\n",
       "      <td>EEEEESSSIIISSSBBBBBBSSSSSSSSSBSSBBBBSSSHHHHHSS...</td>\n",
       "      <td>130</td>\n",
       "      <td>91</td>\n",
       "      <td>[0.5125200536210608, 1.19156364060605, 1.03744...</td>\n",
       "      <td>[0.5863812436307109, 1.434125824290471, 1.0608...</td>\n",
       "      <td>[1.334704451012295, 1.6699436537913352, 1.0838...</td>\n",
       "      <td>[0.5100052155797521, 1.4584923193610004, 1.439...</td>\n",
       "      <td>[0.5048366482863921, 1.1946691243009842, 0.988...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>id_0031191b7</td>\n",
       "      <td>GGAAAUGUCUACAUAGGAGUGCUGCGGGACGGUAACGUCAUGACCG...</td>\n",
       "      <td>........(((((((((((.((..(((((((....))))....)))...</td>\n",
       "      <td>EEEEEEEESSSSSSSSSSSISSIISSSSSSSHHHHSSSSBBBBSSS...</td>\n",
       "      <td>130</td>\n",
       "      <td>91</td>\n",
       "      <td>[0.731134934164374, 1.9836055225668456, 1.5687...</td>\n",
       "      <td>[0.6598995646228629, 2.707940415445323, 0.5863...</td>\n",
       "      <td>[1.7225471233757963, 3.3154216130008005, 0.689...</td>\n",
       "      <td>[0.5218093358218737, 2.899379728487696, 0.7624...</td>\n",
       "      <td>[0.6478372665491631, 2.319938174531894, 0.7588...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>id_003ab2445</td>\n",
       "      <td>GGAAAGACUCAGAGGUGAAGGUCAUCACGGCUGAUAGGAGACUAUC...</td>\n",
       "      <td>.....(((((..........((((((.((.(((((((....)))))...</td>\n",
       "      <td>EEEEESSSSSBBBBBBBBBBSSSSSSISSISSSSSSSHHHHSSSSS...</td>\n",
       "      <td>130</td>\n",
       "      <td>91</td>\n",
       "      <td>[0.7291145234939289, 1.842581802194789, 1.5406...</td>\n",
       "      <td>[0.8392257920695343, 2.091502298209419, 1.5503...</td>\n",
       "      <td>[1.9666465903097312, 2.4865346681802207, 1.302...</td>\n",
       "      <td>[0.6607922392751232, 1.9739020146403017, 1.861...</td>\n",
       "      <td>[0.6407883042621931, 1.4538999646879156, 1.397...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index            id                                           sequence  \\\n",
       "1      1  id_000ae4237  GGAAACGGGUUCCGCGGAUUGCUGCUAAUAAGAGUAAUCUCUAAAU...   \n",
       "4      4  id_0020473f7  GGAAACCCGCCCGCGCCCGCCCGCGCUGCUGCCGUGCCUCCUCUCC...   \n",
       "5      5  id_002852873  GGAAAGCGAAACGCCGAGAAGACGUAGUUCGCAGAGCGGCGUACCC...   \n",
       "6      6  id_0031191b7  GGAAAUGUCUACAUAGGAGUGCUGCGGGACGGUAACGUCAUGACCG...   \n",
       "7      7  id_003ab2445  GGAAAGACUCAGAGGUGAAGGUCAUCACGGCUGAUAGGAGACUAUC...   \n",
       "\n",
       "                                           structure  \\\n",
       "1  .....((((..((((((...(((((.....((((....)))).......   \n",
       "4  .....(((((((((((((((((((((((((((((((((((((((((...   \n",
       "5  .....(((...(((......(((((((((.((....(((.....))...   \n",
       "6  ........(((((((((((.((..(((((((....))))....)))...   \n",
       "7  .....(((((..........((((((.((.(((((((....)))))...   \n",
       "\n",
       "                                 predicted_loop_type  seq_length  seq_scored  \\\n",
       "1  EEEEESSSSIISSSSSSIIISSSSSIIIIISSSSHHHHSSSSIIII...         130          91   \n",
       "4  EEEEESSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS...         130          91   \n",
       "5  EEEEESSSIIISSSBBBBBBSSSSSSSSSBSSBBBBSSSHHHHHSS...         130          91   \n",
       "6  EEEEEEEESSSSSSSSSSSISSIISSSSSSSHHHHSSSSBBBBSSS...         130          91   \n",
       "7  EEEEESSSSSBBBBBBBBBBSSSSSSISSISSSSSSSHHHHSSSSS...         130          91   \n",
       "\n",
       "                                          reactivity  \\\n",
       "1  [0.5988852160599594, 1.252383497027623, 1.1766...   \n",
       "4  [0.751697894404419, 1.8868896799083001, 1.3745...   \n",
       "5  [0.5125200536210608, 1.19156364060605, 1.03744...   \n",
       "6  [0.731134934164374, 1.9836055225668456, 1.5687...   \n",
       "7  [0.7291145234939289, 1.842581802194789, 1.5406...   \n",
       "\n",
       "                                         deg_Mg_pH10  \\\n",
       "1  [0.5802506685595211, 1.9965426237137536, 0.765...   \n",
       "4  [0.7895877059930309, 2.8275117665075093, 0.935...   \n",
       "5  [0.5863812436307109, 1.434125824290471, 1.0608...   \n",
       "6  [0.6598995646228629, 2.707940415445323, 0.5863...   \n",
       "7  [0.8392257920695343, 2.091502298209419, 1.5503...   \n",
       "\n",
       "                                            deg_pH10  \\\n",
       "1  [1.6855349656016847, 1.9856331898502928, 0.841...   \n",
       "4  [1.467933834294549, 2.6756719629474697, 1.0947...   \n",
       "5  [1.334704451012295, 1.6699436537913352, 1.0838...   \n",
       "6  [1.7225471233757963, 3.3154216130008005, 0.689...   \n",
       "7  [1.9666465903097312, 2.4865346681802207, 1.302...   \n",
       "\n",
       "                                          deg_Mg_50C  \\\n",
       "1  [0.4643303444983725, 1.7353359778870612, 0.980...   \n",
       "4  [0.5991996450286899, 2.477975121643563, 1.2246...   \n",
       "5  [0.5100052155797521, 1.4584923193610004, 1.439...   \n",
       "6  [0.5218093358218737, 2.899379728487696, 0.7624...   \n",
       "7  [0.6607922392751232, 1.9739020146403017, 1.861...   \n",
       "\n",
       "                                             deg_50C  \n",
       "1  [0.5961263638596626, 1.330144615692037, 0.9792...  \n",
       "4  [0.5419860902101901, 1.5332587823832124, 1.105...  \n",
       "5  [0.5048366482863921, 1.1946691243009842, 0.988...  \n",
       "6  [0.6478372665491631, 2.319938174531894, 0.7588...  \n",
       "7  [0.6407883042621931, 1.4538999646879156, 1.397...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo_lg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749c499f4aff41439d976b9ca4f3d65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4821.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e11baf6e00407981bbf5fd3c15fe7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3005.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50c1d0f241d4085b6049c3ccba607b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1258.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96194a1ca6924af2bfd57b9c0db3fb35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6010.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "train = pd.read_json(\"../OpenVaccine/train.json\",lines=True)\n",
    "train = aug_data(train)\n",
    "if denoise:\n",
    "    train = train[train.signal_to_noise > 1].reset_index(drop = True)\n",
    "    \n",
    "train = train.append(pseudo_st)\n",
    "train_lg = pseudo_lg\n",
    "test  = pd.read_json(\"../OpenVaccine/test.json\",lines=True)\n",
    "test = aug_data(test)\n",
    "\n",
    "test_pub = test[test[\"seq_length\"] == 107]\n",
    "test_pri = test[test[\"seq_length\"] == 130]\n",
    "sub = pd.read_csv(\"../OpenVaccine/sample_submission.csv\")\n",
    "\n",
    "if run_test: ## to test \n",
    "    train = train[:30]\n",
    "    test_pub = test_pub[:30]\n",
    "    test_pri = test_pri[:30]\n",
    "\n",
    "As = []\n",
    "for id in tqdm(train[\"id\"]):\n",
    "    a = np.load(f\"../OpenVaccine/bpps/{id}.npy\")\n",
    "    As.append(a)\n",
    "As = np.array(As)\n",
    "\n",
    "As_lg = []\n",
    "for id in tqdm(train_lg[\"id\"]):\n",
    "    a = np.load(f\"../OpenVaccine/bpps/{id}.npy\")\n",
    "    As_lg.append(a)\n",
    "As_lg = np.array(As_lg)\n",
    "\n",
    "As_pub = []\n",
    "for id in tqdm(test_pub[\"id\"]):\n",
    "    a = np.load(f\"../OpenVaccine/bpps/{id}.npy\")\n",
    "    As_pub.append(a)\n",
    "As_pub = np.array(As_pub)\n",
    "\n",
    "As_pri = []\n",
    "for id in tqdm(test_pri[\"id\"]):\n",
    "    a = np.load(f\"../OpenVaccine/bpps/{id}.npy\")\n",
    "    As_pri.append(a)\n",
    "As_pri = np.array(As_pri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n",
      "(4821, 107, 5)\n",
      "(3005, 130, 5)\n"
     ]
    }
   ],
   "source": [
    "targets = list(sub.columns[1:])\n",
    "print(targets)\n",
    "\n",
    "y_train = []\n",
    "seq_len = train[\"seq_length\"].iloc[0]\n",
    "seq_len_target = train[\"seq_scored\"].iloc[0]\n",
    "ignore = -10000\n",
    "ignore_length = seq_len - seq_len_target\n",
    "for target in targets:\n",
    "    y_tmp = np.vstack(train[target])\n",
    "    dummy = np.zeros([y_tmp.shape[0], ignore_length]) + ignore\n",
    "    y_tmp = np.hstack([y_tmp, dummy])\n",
    "    y_train.append(y_tmp)\n",
    "y = np.stack(y_train, axis = 2)\n",
    "print(y.shape)\n",
    "\n",
    "y_train = []\n",
    "seq_len = train_lg[\"seq_length\"].iloc[0]\n",
    "seq_len_target = train_lg[\"seq_scored\"].iloc[0]\n",
    "ignore = -10000\n",
    "ignore_length = seq_len - seq_len_target\n",
    "for target in targets:\n",
    "    y_tmp = np.vstack(train_lg[target])\n",
    "    dummy = np.zeros([y_tmp.shape[0], ignore_length]) + ignore\n",
    "    y_tmp = np.hstack([y_tmp, dummy])\n",
    "    y_train.append(y_tmp)\n",
    "y_lg = np.stack(y_train, axis = 2)\n",
    "print(y_lg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## structure adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb68669e8e0644f6b7b0a78c735f3295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4821.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(4821, 107, 107, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6afb7d86c547bd9d6290de15a9c433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3005.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(3005, 130, 130, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308e85cd484f43c295800e9c4139dcdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1258.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(1258, 107, 107, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e1a7cfcc584838a4247f0188903238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6010.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(6010, 130, 130, 1)\n"
     ]
    }
   ],
   "source": [
    "def get_structure_adj(train):\n",
    "    ## get adjacent matrix from structure sequence\n",
    "    \n",
    "    ## here I calculate adjacent matrix of each base pair, \n",
    "    ## but eventually ignore difference of base pair and integrate into one matrix\n",
    "    Ss = []\n",
    "    for i in tqdm(range(len(train))):\n",
    "        seq_length = train[\"seq_length\"].iloc[i]\n",
    "        structure = train[\"structure\"].iloc[i]\n",
    "        sequence = train[\"sequence\"].iloc[i]\n",
    "\n",
    "        cue = []\n",
    "        a_structures = {\n",
    "            (\"A\", \"U\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"C\", \"G\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"U\", \"G\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"U\", \"A\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"G\", \"C\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"G\", \"U\") : np.zeros([seq_length, seq_length]),\n",
    "        }\n",
    "        a_structure = np.zeros([seq_length, seq_length])\n",
    "        for i in range(seq_length):\n",
    "            if structure[i] == \"(\":\n",
    "                cue.append(i)\n",
    "            elif structure[i] == \")\":\n",
    "                start = cue.pop()\n",
    "#                 a_structure[start, i] = 1\n",
    "#                 a_structure[i, start] = 1\n",
    "                a_structures[(sequence[start], sequence[i])][start, i] = 1\n",
    "                a_structures[(sequence[i], sequence[start])][i, start] = 1\n",
    "        \n",
    "        a_strc = np.stack([a for a in a_structures.values()], axis = 2)\n",
    "        a_strc = np.sum(a_strc, axis = 2, keepdims = True)\n",
    "        Ss.append(a_strc)\n",
    "    \n",
    "    Ss = np.array(Ss)\n",
    "    print(Ss.shape)\n",
    "    return Ss\n",
    "Ss = get_structure_adj(train)\n",
    "Ss_lg = get_structure_adj(train_lg)\n",
    "Ss_pub = get_structure_adj(test_pub)\n",
    "Ss_pri = get_structure_adj(test_pri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distance adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4821, 107, 107, 3)\n",
      "(3005, 130, 130, 3)\n",
      "(1258, 107, 107, 3)\n",
      "(6010, 130, 130, 3)\n"
     ]
    }
   ],
   "source": [
    "def get_distance_matrix(As):\n",
    "    ## adjacent matrix based on distance on the sequence\n",
    "    ## D[i, j] = 1 / (abs(i - j) + 1) ** pow, pow = 1, 2, 4\n",
    "    \n",
    "    idx = np.arange(As.shape[1])\n",
    "    Ds = []\n",
    "    for i in range(len(idx)):\n",
    "        d = np.abs(idx[i] - idx)\n",
    "        Ds.append(d)\n",
    "\n",
    "    Ds = np.array(Ds) + 1\n",
    "    Ds = 1/Ds\n",
    "    Ds = Ds[None, :,:]\n",
    "    Ds = np.repeat(Ds, len(As), axis = 0)\n",
    "    \n",
    "    Dss = []\n",
    "    for i in [1, 2, 4]: \n",
    "        Dss.append(Ds ** i)\n",
    "    Ds = np.stack(Dss, axis = 3)\n",
    "    print(Ds.shape)\n",
    "    return Ds\n",
    "\n",
    "Ds = get_distance_matrix(As)\n",
    "Ds_lg = get_distance_matrix(As_lg)\n",
    "Ds_pub = get_distance_matrix(As_pub)\n",
    "Ds_pri = get_distance_matrix(As_pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4821, 107, 107, 5),\n",
       " (3005, 130, 130, 5),\n",
       " (1258, 107, 107, 5),\n",
       " (6010, 130, 130, 5))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## concat adjecent\n",
    "As = np.concatenate([As[:,:,:,None], Ss, Ds], axis = 3).astype(np.float32)\n",
    "As_lg = np.concatenate([As_lg[:,:,:,None], Ss_lg, Ds_lg], axis = 3).astype(np.float32)\n",
    "\n",
    "As_pub = np.concatenate([As_pub[:,:,:,None], Ss_pub, Ds_pub], axis = 3).astype(np.float32)\n",
    "As_pri = np.concatenate([As_pri[:,:,:,None], Ss_pri, Ds_pri], axis = 3).astype(np.float32)\n",
    "del Ss, Ds, Ss_lg, Ds_lg, Ss_pub, Ds_pub, Ss_pri, Ds_pri\n",
    "As.shape, As_lg.shape, As_pub.shape, As_pri.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 18, 20, 24, 33, 34, 36, 40, 65, 66, 68, 72, 129, 130, 132, 136, 257, 258, 260, 264, 513, 514, 516, 520, 1025, 1026, 1028, 1032]\n",
      "(4821, 107, 39)\n",
      "[17, 18, 20, 24, 33, 34, 36, 40, 65, 66, 68, 72, 129, 130, 132, 136, 257, 258, 260, 264, 513, 514, 516, 520, 1025, 1026, 1028, 1032]\n",
      "(3005, 130, 39)\n",
      "[17, 18, 20, 24, 33, 34, 36, 40, 65, 66, 68, 72, 129, 130, 132, 136, 257, 258, 260, 264, 513, 514, 516, 520, 1025, 1026, 1028, 1032]\n",
      "(1258, 107, 39)\n",
      "[17, 18, 20, 24, 33, 34, 36, 40, 65, 66, 68, 72, 129, 130, 132, 136, 257, 258, 260, 264, 513, 514, 516, 520, 1025, 1026, 1028, 1032]\n",
      "(6010, 130, 39)\n"
     ]
    }
   ],
   "source": [
    "## sequence\n",
    "def return_ohe(n, i):\n",
    "    tmp = [0] * n\n",
    "    tmp[i] = 1\n",
    "    return tmp\n",
    "\n",
    "def get_input(train):\n",
    "    ## get node features, which is one hot encoded\n",
    "    mapping = {}\n",
    "    vocab = [\"A\", \"G\", \"C\", \"U\"]\n",
    "    for i, s in enumerate(vocab):\n",
    "        mapping[s] = return_ohe(len(vocab), i)\n",
    "    X_node = np.stack(train[\"sequence\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n",
    "\n",
    "    mapping = {}\n",
    "    vocab = [\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"]\n",
    "    for i, s in enumerate(vocab):\n",
    "        mapping[s] = return_ohe(len(vocab), i)\n",
    "    X_loop = np.stack(train[\"predicted_loop_type\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n",
    "    \n",
    "    mapping = {}\n",
    "    vocab = [\".\", \"(\", \")\"]\n",
    "    for i, s in enumerate(vocab):\n",
    "        mapping[s] = return_ohe(len(vocab), i)\n",
    "    X_structure = np.stack(train[\"structure\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n",
    "    \n",
    "    \n",
    "    X_node = np.concatenate([X_node, X_loop], axis = 2)\n",
    "    \n",
    "    ## interaction\n",
    "    a = np.sum(X_node * (2 ** np.arange(X_node.shape[2])[None, None, :]), axis = 2)\n",
    "    vocab = sorted(set(a.flatten()))\n",
    "    print(vocab)\n",
    "    ohes = []\n",
    "    for v in vocab:\n",
    "        ohes.append(a == v)\n",
    "    ohes = np.stack(ohes, axis = 2)\n",
    "    X_node = np.concatenate([X_node, ohes], axis = 2).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    print(X_node.shape)\n",
    "    return X_node\n",
    "\n",
    "X_node = get_input(train)\n",
    "X_node_lg = get_input(train_lg)\n",
    "X_node_pub = get_input(test_pub)\n",
    "X_node_pri = get_input(test_pri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as L\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def mcrmse(t, p):\n",
    "    ## calculate mcrmse score by using numpy\n",
    "    if t.shape[1] == 107:\n",
    "        seq_len_target = 68\n",
    "    elif t.shape[1] == 130:\n",
    "        seq_len_target = 91\n",
    "        \n",
    "    t = t[:, :seq_len_target]\n",
    "    p = p[:, :seq_len_target]\n",
    "    \n",
    "    score = np.mean(np.sqrt(np.mean(np.mean((p - t) ** 2, axis = 1), axis = 0)))\n",
    "    return score\n",
    "\n",
    "def mcrmse_loss(t, y):\n",
    "    if t.shape[1] == 107:\n",
    "        seq_len_target = 68\n",
    "    elif t.shape[1] == 130:\n",
    "        seq_len_target = 91\n",
    "        \n",
    "    ## calculate mcrmse score by using tf\n",
    "    t = t[:, :seq_len_target]\n",
    "    y = y[:, :seq_len_target]\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.sqrt(tf.reduce_mean(tf.reduce_mean((t - y) ** 2, axis = 1), axis = 0)))\n",
    "    return loss\n",
    "\n",
    "def attention(x_inner, x_outer, n_factor, dropout):\n",
    "    x_Q =  L.Conv1D(n_factor, 1, activation='linear', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  bias_initializer='glorot_uniform',\n",
    "                 )(x_inner)\n",
    "    x_K =  L.Conv1D(n_factor, 1, activation='linear', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  bias_initializer='glorot_uniform',\n",
    "                 )(x_outer)\n",
    "    x_V =  L.Conv1D(n_factor, 1, activation='linear', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  bias_initializer='glorot_uniform',\n",
    "                 )(x_outer)\n",
    "    x_KT = L.Permute((2, 1))(x_K)\n",
    "    res = L.Lambda(lambda c: K.batch_dot(c[0], c[1]) / np.sqrt(n_factor))([x_Q, x_KT])\n",
    "#     res = tf.expand_dims(res, axis = 3)\n",
    "#     res = L.Conv2D(16, 3, 1, padding = \"same\", activation = \"relu\")(res)\n",
    "#     res = L.Conv2D(1, 3, 1, padding = \"same\", activation = \"relu\")(res)\n",
    "#     res = tf.squeeze(res, axis = 3)\n",
    "    att = L.Lambda(lambda c: K.softmax(c, axis=-1))(res)\n",
    "    att = L.Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n",
    "    return att\n",
    "\n",
    "def multi_head_attention(x, y, n_factor, n_head, dropout):\n",
    "    if n_head == 1:\n",
    "        att = attention(x, y, n_factor, dropout)\n",
    "    else:\n",
    "        n_factor_head = n_factor // n_head\n",
    "        heads = [attention(x, y, n_factor_head, dropout) for i in range(n_head)]\n",
    "        att = L.Concatenate()(heads)\n",
    "        att = L.Dense(n_factor, \n",
    "                      kernel_initializer='glorot_uniform',\n",
    "                      bias_initializer='glorot_uniform',\n",
    "                     )(att)\n",
    "    x = L.Add()([x, att])\n",
    "    x = L.LayerNormalization()(x)\n",
    "    if dropout > 0:\n",
    "        x = L.Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "def res(x, unit, kernel = 3, rate = 0.1):\n",
    "    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n",
    "    h = L.LayerNormalization()(h)\n",
    "    h = L.LeakyReLU()(h)\n",
    "    h = L.Dropout(rate)(h)\n",
    "    return L.Add()([x, h])\n",
    "\n",
    "def forward(x, unit, kernel = 3, rate = 0.1):\n",
    "#     h = L.Dense(unit, None)(x)\n",
    "    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n",
    "    h = L.LayerNormalization()(h)\n",
    "    h = L.Dropout(rate)(h)\n",
    "#         h = tf.keras.activations.swish(h)\n",
    "    h = L.LeakyReLU()(h)\n",
    "    h = res(h, unit, kernel, rate)\n",
    "    return h\n",
    "\n",
    "def adj_attn(x, adj, unit, n = 2, rate = 0.1):\n",
    "    x_a = x\n",
    "    x_as = []\n",
    "    for i in range(n):\n",
    "        x_a = forward(x_a, unit)\n",
    "        x_a = tf.matmul(adj, x_a) ## aggregate neighborhoods\n",
    "        x_as.append(x_a)\n",
    "    if n == 1:\n",
    "        x_a = x_as[0]\n",
    "    else:\n",
    "        x_a = L.Concatenate()(x_as)\n",
    "    x_a = forward(x_a, unit)\n",
    "    return x_a\n",
    "\n",
    "\n",
    "def get_base(config):\n",
    "    ## base model architecture \n",
    "    ## node, adj -> middle feature\n",
    "    \n",
    "    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n",
    "    adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n",
    "    \n",
    "    adj_learned = L.Dense(1, \"relu\")(adj)\n",
    "    adj_all = L.Concatenate(axis = 3)([adj, adj_learned])\n",
    "        \n",
    "    xs = []\n",
    "    xs.append(node)\n",
    "    x1 = forward(node, 128, kernel = 3, rate = 0.0)\n",
    "    x2 = forward(x1, 64, kernel = 6, rate = 0.0)\n",
    "    x3 = forward(x2, 32, kernel = 15, rate = 0.0)\n",
    "    x4 = forward(x3, 16, kernel = 30, rate = 0.0)\n",
    "    x = L.Concatenate()([x1, x2, x3, x4])\n",
    "    \n",
    "    for unit in [64, 32]:\n",
    "        x_as = []\n",
    "        for i in range(adj_all.shape[3]):\n",
    "            x_a = adj_attn(x, adj_all[:, :, :, i], unit, rate = 0.0)\n",
    "            x_as.append(x_a)\n",
    "        x_c = forward(x, unit, kernel = 30)\n",
    "        x_c = forward(x_c, unit*2, kernel = 15)\n",
    "        x_c = forward(x_c, unit*3, kernel = 6)\n",
    "        \n",
    "        x = L.Concatenate()(x_as + [x_c])\n",
    "        x = forward(x, unit)\n",
    "        x = multi_head_attention(x, x, unit, 4, 0.0)\n",
    "        xs.append(x)\n",
    "        \n",
    "    x = L.Concatenate()(xs)\n",
    "\n",
    "    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ae_model(base, config):\n",
    "    ## denoising auto encoder part\n",
    "    ## node, adj -> middle feature -> node\n",
    "    \n",
    "    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n",
    "    adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n",
    "\n",
    "    x = base([L.SpatialDropout1D(0.3)(node), adj])\n",
    "    x = forward(x, 64, rate = 0.3)\n",
    "    p = L.Dense(X_node.shape[2], \"sigmoid\")(x)\n",
    "    \n",
    "    loss = - tf.reduce_mean(20 * node * tf.math.log(p + 1e-4) + (1 - node) * tf.math.log(1 - p + 1e-4))\n",
    "    model = tf.keras.Model(inputs = [node, adj], outputs = [loss])\n",
    "    \n",
    "    opt = get_optimizer()\n",
    "    model.compile(optimizer = opt, loss = lambda t, y : y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model(base, config):\n",
    "    ## regression part\n",
    "    ## node, adj -> middle feature -> prediction of targets\n",
    "    \n",
    "    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n",
    "    adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n",
    "    \n",
    "    x = base([node, adj])\n",
    "    x = forward(x, 128, rate = 0.4)\n",
    "    x = L.Dense(5, None)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n",
    "    \n",
    "    opt = get_optimizer()\n",
    "    model.compile(optimizer = opt, loss = mcrmse_loss)\n",
    "    return model\n",
    "\n",
    "def get_optimizer():\n",
    "#     sgd = tf.keras.optimizers.SGD(0.05, momentum = 0.9, nesterov=True)\n",
    "    adam = tf.optimizers.Adam()\n",
    "#     radam = tfa.optimizers.RectifiedAdam()\n",
    "#     lookahead = tfa.optimizers.Lookahead(adam, sync_period=6)\n",
    "#     swa = tfa.optimizers.SWA(adam)\n",
    "    return adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ 0 ------\n",
      "--- train ---\n",
      "Epoch 1/5\n",
      "151/151 [==============================] - 12s 76ms/step - loss: 0.5708\n",
      "Epoch 2/5\n",
      "151/151 [==============================] - 10s 68ms/step - loss: 0.1266\n",
      "Epoch 3/5\n",
      "151/151 [==============================] - 10s 68ms/step - loss: 0.0701\n",
      "Epoch 4/5\n",
      "151/151 [==============================] - 10s 68ms/step - loss: 0.0509\n",
      "Epoch 5/5\n",
      "151/151 [==============================] - 10s 68ms/step - loss: 0.0387\n",
      "--- train long ---\n",
      "Epoch 1/5\n",
      "94/94 [==============================] - 9s 93ms/step - loss: 0.0335\n",
      "Epoch 2/5\n",
      "94/94 [==============================] - 8s 80ms/step - loss: 0.0305\n",
      "Epoch 3/5\n",
      "94/94 [==============================] - 8s 80ms/step - loss: 0.0293\n",
      "Epoch 4/5\n",
      "94/94 [==============================] - 8s 80ms/step - loss: 0.0256\n",
      "Epoch 5/5\n",
      "94/94 [==============================] - 7s 80ms/step - loss: 0.0215\n",
      "--- public ---\n",
      "Epoch 1/5\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 0.0274\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.0203\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.0225\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.0177\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.0159\n",
      "--- private ---\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 16s 87ms/step - loss: 0.0192\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 15s 81ms/step - loss: 0.0177\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 15s 80ms/step - loss: 0.0158\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 15s 80ms/step - loss: 0.0142\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 15s 80ms/step - loss: 0.0153\n",
      "------ 1 ------\n",
      "--- train ---\n",
      "Epoch 1/5\n",
      "151/151 [==============================] - 10s 69ms/step - loss: 0.0149\n",
      "Epoch 2/5\n",
      "151/151 [==============================] - 10s 69ms/step - loss: 0.0145\n",
      "Epoch 3/5\n",
      "151/151 [==============================] - 10s 69ms/step - loss: 0.0122\n",
      "Epoch 4/5\n",
      "151/151 [==============================] - 10s 69ms/step - loss: 0.0105\n",
      "Epoch 5/5\n",
      "151/151 [==============================] - 10s 69ms/step - loss: 0.0101\n",
      "--- train long ---\n",
      "Epoch 1/5\n",
      "94/94 [==============================] - 8s 80ms/step - loss: 0.0103\n",
      "Epoch 2/5\n",
      "94/94 [==============================] - 8s 80ms/step - loss: 0.0089\n",
      "Epoch 3/5\n",
      "94/94 [==============================] - 8s 80ms/step - loss: 0.0082\n",
      "Epoch 4/5\n",
      "94/94 [==============================] - 8s 80ms/step - loss: 0.0099\n",
      "Epoch 5/5\n",
      "94/94 [==============================] - 8s 80ms/step - loss: 0.0088\n",
      "--- public ---\n",
      "Epoch 1/5\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.0082\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.0075\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.0092\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.0102\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.0102\n",
      "--- private ---\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 15s 81ms/step - loss: 0.0096\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 15s 81ms/step - loss: 0.0100\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 15s 81ms/step - loss: 0.0087\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 15s 81ms/step - loss: 0.0089\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 15s 81ms/step - loss: 0.0090\n",
      "------ 2 ------\n",
      "--- train ---\n",
      "Epoch 1/5\n",
      "151/151 [==============================] - 10s 69ms/step - loss: 0.0076\n",
      "Epoch 2/5\n",
      "151/151 [==============================] - 10s 69ms/step - loss: 0.0062\n",
      "Epoch 3/5\n",
      "151/151 [==============================] - 10s 70ms/step - loss: 0.0082\n",
      "Epoch 4/5\n",
      "151/151 [==============================] - 11s 70ms/step - loss: 0.0072\n",
      "Epoch 5/5\n",
      "151/151 [==============================] - 11s 70ms/step - loss: 0.0069\n",
      "--- train long ---\n",
      "Epoch 1/5\n",
      "94/94 [==============================] - 8s 81ms/step - loss: 0.0071\n",
      "Epoch 2/5\n",
      "94/94 [==============================] - 8s 82ms/step - loss: 0.0070\n",
      "Epoch 3/5\n",
      "94/94 [==============================] - 8s 82ms/step - loss: 0.0061\n",
      "Epoch 4/5\n",
      "94/94 [==============================] - 8s 82ms/step - loss: 0.0063\n",
      "Epoch 5/5\n",
      "94/94 [==============================] - 8s 82ms/step - loss: 0.0054\n",
      "--- public ---\n",
      "Epoch 1/5\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.0048\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.0055\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.0048\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.0056\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.0048\n",
      "--- private ---\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 15s 82ms/step - loss: 0.0058\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 15s 82ms/step - loss: 0.0064\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 16s 83ms/step - loss: 0.0065\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 16s 83ms/step - loss: 0.0054\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 15s 82ms/step - loss: 0.0070\n",
      "------ 3 ------\n",
      "--- train ---\n",
      "Epoch 1/5\n",
      "151/151 [==============================] - 11s 71ms/step - loss: 0.0055\n",
      "Epoch 2/5\n",
      "151/151 [==============================] - 11s 71ms/step - loss: 0.0059\n",
      "Epoch 3/5\n",
      "151/151 [==============================] - 11s 71ms/step - loss: 0.0047\n",
      "Epoch 4/5\n",
      "151/151 [==============================] - 11s 71ms/step - loss: 0.0040\n",
      "Epoch 5/5\n",
      "151/151 [==============================] - 11s 71ms/step - loss: 0.0046\n",
      "--- train long ---\n",
      "Epoch 1/5\n",
      "94/94 [==============================] - 8s 81ms/step - loss: 0.0048\n",
      "Epoch 2/5\n",
      "94/94 [==============================] - 8s 84ms/step - loss: 0.0056\n",
      "Epoch 3/5\n",
      "94/94 [==============================] - 8s 83ms/step - loss: 0.0058\n",
      "Epoch 4/5\n",
      "94/94 [==============================] - 8s 83ms/step - loss: 0.0048\n",
      "Epoch 5/5\n",
      "94/94 [==============================] - 8s 83ms/step - loss: 0.0046\n",
      "--- public ---\n",
      "Epoch 1/5\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.0046\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.0036\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.0047\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.0032\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.0037\n",
      "--- private ---\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 16s 83ms/step - loss: 0.0054\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 16s 83ms/step - loss: 0.0046\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 16s 83ms/step - loss: 0.0044\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 16s 84ms/step - loss: 0.0052\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 16s 84ms/step - loss: 0.0047\n",
      "****** save ae model ******\n"
     ]
    }
   ],
   "source": [
    "## here train denoising auto encoder model using all data\n",
    "\n",
    "config = {} ## not use now\n",
    "if ae_epochs > 0:\n",
    "    base = get_base(config)\n",
    "    ae_model = get_ae_model(base, config)\n",
    "    ## TODO : simultaneous train\n",
    "    for i in range(ae_epochs//ae_epochs_each):\n",
    "        print(f\"------ {i} ------\")\n",
    "        print(\"--- train ---\")\n",
    "        ae_model.fit([X_node, As], [X_node[:,0]],\n",
    "                  epochs = ae_epochs_each,\n",
    "                  batch_size = ae_batch_size)\n",
    "        print(\"--- train long ---\")\n",
    "        ae_model.fit([X_node_lg, As_lg], [X_node_lg[:,0]],\n",
    "                  epochs = ae_epochs_each,\n",
    "                  batch_size = ae_batch_size)\n",
    "        print(\"--- public ---\")\n",
    "        ae_model.fit([X_node_pub, As_pub], [X_node_pub[:,0]],\n",
    "                  epochs = ae_epochs_each,\n",
    "                  batch_size = ae_batch_size)\n",
    "        print(\"--- private ---\")\n",
    "        ae_model.fit([X_node_pri, As_pri], [X_node_pri[:,0]],\n",
    "                  epochs = ae_epochs_each,\n",
    "                  batch_size = ae_batch_size)\n",
    "        gc.collect()\n",
    "    print(\"****** save ae model ******\")\n",
    "    base.save_weights(\"./base_ae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ fold 0 start -----\n",
      "------ fold 0 start -----\n",
      "------ fold 0 start -----\n",
      "****** load ae model ******\n",
      "epochs : 30, batch_size : 8\n",
      "301/301 [==============================] - 13s 43ms/step - loss: 0.3547\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.3016\n",
      "301/301 [==============================] - 12s 40ms/step - loss: 0.1316\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.2664\n",
      "301/301 [==============================] - 12s 40ms/step - loss: 0.1096\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.2499\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.1012\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.2430\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0946\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.2353\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0915\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.2306\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0879\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.2256\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0851\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.2212\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0835\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.2176\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0820\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.2140\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0799\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.2103\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0788\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.2077\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0778\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.2062\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0764\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.2028\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0753\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.2010\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0747\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.1988\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0748\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.1960\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0733\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.1948\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0732\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.1924\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0720\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.1928\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0716\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.1903\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0715\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.1882\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0705\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.1869\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0703\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.1849\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0701\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.1839\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0691\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.1838\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0688\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.1816\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0681\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.1806\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0680\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.1799\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0678\n",
      "482/482 [==============================] - 17s 36ms/step - loss: 0.1783\n",
      "epochs : 10, batch_size : 16\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0673\n",
      "241/241 [==============================] - 12s 49ms/step - loss: 0.1732\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0658\n",
      "241/241 [==============================] - 12s 49ms/step - loss: 0.1723\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0657\n",
      "241/241 [==============================] - 12s 49ms/step - loss: 0.1716\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0664\n",
      "241/241 [==============================] - 12s 49ms/step - loss: 0.1716\n",
      "151/151 [==============================] - 8s 55ms/step - loss: 0.0656\n",
      "241/241 [==============================] - 12s 50ms/step - loss: 0.1709\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0656\n",
      "241/241 [==============================] - 12s 50ms/step - loss: 0.1701\n",
      "151/151 [==============================] - 8s 55ms/step - loss: 0.0654\n",
      "241/241 [==============================] - 12s 50ms/step - loss: 0.1698\n",
      "151/151 [==============================] - 8s 55ms/step - loss: 0.0650\n",
      "241/241 [==============================] - 12s 50ms/step - loss: 0.1689\n",
      "151/151 [==============================] - 8s 55ms/step - loss: 0.0655\n",
      "241/241 [==============================] - 12s 50ms/step - loss: 0.1689\n",
      "151/151 [==============================] - 8s 55ms/step - loss: 0.0649\n",
      "241/241 [==============================] - 12s 50ms/step - loss: 0.1679\n",
      "epochs : 5, batch_size : 32\n",
      "76/76 [==============================] - 6s 84ms/step - loss: 0.0659\n",
      "121/121 [==============================] - 9s 73ms/step - loss: 0.1654\n",
      "76/76 [==============================] - 6s 81ms/step - loss: 0.0644\n",
      "121/121 [==============================] - 9s 73ms/step - loss: 0.1652\n",
      "76/76 [==============================] - 6s 84ms/step - loss: 0.0649\n",
      "121/121 [==============================] - 9s 73ms/step - loss: 0.1642\n",
      "76/76 [==============================] - 6s 80ms/step - loss: 0.0646\n",
      "121/121 [==============================] - 9s 74ms/step - loss: 0.1636\n",
      "76/76 [==============================] - 6s 84ms/step - loss: 0.0646\n",
      "121/121 [==============================] - 9s 73ms/step - loss: 0.1637\n",
      "epochs : 5, batch_size : 64\n",
      "38/38 [==============================] - 7s 173ms/step - loss: 0.0680\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1632\n",
      "38/38 [==============================] - 5s 133ms/step - loss: 0.0671\n",
      "61/61 [==============================] - 8s 123ms/step - loss: 0.1626\n",
      "38/38 [==============================] - 5s 134ms/step - loss: 0.0662\n",
      "61/61 [==============================] - 7s 119ms/step - loss: 0.1624\n",
      "38/38 [==============================] - 5s 132ms/step - loss: 0.0662\n",
      "61/61 [==============================] - 7s 118ms/step - loss: 0.1623\n",
      "38/38 [==============================] - 5s 133ms/step - loss: 0.0659\n",
      "61/61 [==============================] - 7s 122ms/step - loss: 0.1619\n",
      "epochs : 8, batch_size : 128\n",
      "19/19 [==============================] - 5s 288ms/step - loss: 0.0696\n",
      "31/31 [==============================] - 6s 189ms/step - loss: 0.1645\n",
      "19/19 [==============================] - 5s 253ms/step - loss: 0.0704\n",
      "31/31 [==============================] - 6s 198ms/step - loss: 0.1646\n",
      "19/19 [==============================] - 5s 243ms/step - loss: 0.0701\n",
      "31/31 [==============================] - 6s 197ms/step - loss: 0.1645\n",
      "19/19 [==============================] - 4s 234ms/step - loss: 0.0693\n",
      "31/31 [==============================] - 6s 197ms/step - loss: 0.1641\n",
      "19/19 [==============================] - 5s 240ms/step - loss: 0.0696\n",
      "31/31 [==============================] - 6s 196ms/step - loss: 0.1640 1s - loss:\n",
      "19/19 [==============================] - 5s 249ms/step - loss: 0.0694\n",
      "31/31 [==============================] - 6s 197ms/step - loss: 0.1633\n",
      "19/19 [==============================] - 5s 252ms/step - loss: 0.0694\n",
      "31/31 [==============================] - 6s 192ms/step - loss: 0.1629\n",
      "19/19 [==============================] - 5s 244ms/step - loss: 0.0691\n",
      "31/31 [==============================] - 6s 194ms/step - loss: 0.1631\n",
      "epochs : 8, batch_size : 256\n",
      "10/10 [==============================] - 4s 371ms/step - loss: 0.0729\n",
      "16/16 [==============================] - 5s 322ms/step - loss: 0.1676\n",
      "10/10 [==============================] - 4s 397ms/step - loss: 0.0735\n",
      "16/16 [==============================] - 5s 331ms/step - loss: 0.1663\n",
      "10/10 [==============================] - 4s 405ms/step - loss: 0.0718\n",
      "16/16 [==============================] - 5s 326ms/step - loss: 0.1651\n",
      "10/10 [==============================] - 4s 411ms/step - loss: 0.0715\n",
      "16/16 [==============================] - 5s 332ms/step - loss: 0.1657\n",
      "10/10 [==============================] - 4s 389ms/step - loss: 0.0714\n",
      "16/16 [==============================] - 5s 334ms/step - loss: 0.1652\n",
      "10/10 [==============================] - 4s 396ms/step - loss: 0.0726\n",
      "16/16 [==============================] - 5s 339ms/step - loss: 0.1658\n",
      "10/10 [==============================] - 4s 407ms/step - loss: 0.0723\n",
      "16/16 [==============================] - 5s 328ms/step - loss: 0.1652\n",
      "10/10 [==============================] - 4s 390ms/step - loss: 0.0737\n",
      "16/16 [==============================] - 6s 345ms/step - loss: 0.1672\n",
      "fold 0: mcrmse 0.20758395855145823 mcrmse pseudo 0.08172962266083396\n",
      "------ fold 1 start -----\n",
      "------ fold 1 start -----\n",
      "------ fold 1 start -----\n",
      "****** load ae model ******\n",
      "epochs : 30, batch_size : 8\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.3671\n",
      "483/483 [==============================] - 19s 40ms/step - loss: 0.3048\n",
      "301/301 [==============================] - 12s 42ms/step - loss: 0.1348\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.2674\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.1131\n",
      "483/483 [==============================] - 20s 41ms/step - loss: 0.2518\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.1022\n",
      "483/483 [==============================] - 20s 42ms/step - loss: 0.2432\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0963\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.2366\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0914\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.2306\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0890\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.2265\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0864\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.2218\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0841\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.2168\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0821\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.2146\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0812\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.2115\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0797\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.2081\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0785\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.2043\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0777\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.2025\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0761\n",
      "483/483 [==============================] - 19s 40ms/step - loss: 0.2000\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0757\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.1983\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0741\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.1968\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0745\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.1947\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0731\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.1927\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0725\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.1905\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0720\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.1889\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0715\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.1878\n",
      "301/301 [==============================] - 12s 42ms/step - loss: 0.0709\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.1864\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0710\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.1849\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0705\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.1840\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0695\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.1817\n",
      "301/301 [==============================] - 13s 44ms/step - loss: 0.0694\n",
      "483/483 [==============================] - 19s 40ms/step - loss: 0.1816\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0689\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.1790\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0692\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.1786\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0681\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.1778\n",
      "epochs : 10, batch_size : 16\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0676\n",
      "242/242 [==============================] - 12s 49ms/step - loss: 0.1727\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0667\n",
      "242/242 [==============================] - 12s 49ms/step - loss: 0.1719\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0667\n",
      "242/242 [==============================] - 12s 49ms/step - loss: 0.1713\n",
      "151/151 [==============================] - 8s 55ms/step - loss: 0.0656\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1706 0s\n",
      "151/151 [==============================] - 8s 55ms/step - loss: 0.0657\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1708\n",
      "151/151 [==============================] - 8s 55ms/step - loss: 0.0656\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1690\n",
      "151/151 [==============================] - 8s 55ms/step - loss: 0.0656\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1691\n",
      "151/151 [==============================] - 8s 55ms/step - loss: 0.0657\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1688\n",
      "151/151 [==============================] - 8s 55ms/step - loss: 0.0662\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1678\n",
      "151/151 [==============================] - 8s 55ms/step - loss: 0.0661\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1674\n",
      "epochs : 5, batch_size : 32\n",
      "76/76 [==============================] - 6s 81ms/step - loss: 0.0668\n",
      "121/121 [==============================] - 10s 83ms/step - loss: 0.1643\n",
      "76/76 [==============================] - 6s 83ms/step - loss: 0.0651\n",
      "121/121 [==============================] - 9s 72ms/step - loss: 0.1638\n",
      "76/76 [==============================] - 6s 81ms/step - loss: 0.0651\n",
      "121/121 [==============================] - 9s 73ms/step - loss: 0.1634\n",
      "76/76 [==============================] - 6s 82ms/step - loss: 0.0648\n",
      "121/121 [==============================] - 9s 73ms/step - loss: 0.1627\n",
      "76/76 [==============================] - 6s 83ms/step - loss: 0.0646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121/121 [==============================] - 9s 72ms/step - loss: 0.1625\n",
      "epochs : 5, batch_size : 64\n",
      "38/38 [==============================] - 5s 135ms/step - loss: 0.0673\n",
      "61/61 [==============================] - 7s 116ms/step - loss: 0.1621\n",
      "38/38 [==============================] - 5s 137ms/step - loss: 0.0666\n",
      "61/61 [==============================] - 7s 115ms/step - loss: 0.1616\n",
      "38/38 [==============================] - 5s 134ms/step - loss: 0.0665\n",
      "61/61 [==============================] - 7s 119ms/step - loss: 0.1616\n",
      "38/38 [==============================] - 5s 136ms/step - loss: 0.0664\n",
      "61/61 [==============================] - 7s 116ms/step - loss: 0.1610\n",
      "38/38 [==============================] - 5s 131ms/step - loss: 0.0655\n",
      "61/61 [==============================] - 7s 120ms/step - loss: 0.1616\n",
      "epochs : 8, batch_size : 128\n",
      "19/19 [==============================] - 4s 230ms/step - loss: 0.0704\n",
      "31/31 [==============================] - 6s 193ms/step - loss: 0.1640\n",
      "19/19 [==============================] - 4s 231ms/step - loss: 0.0700\n",
      "31/31 [==============================] - 6s 196ms/step - loss: 0.1635 1s - los\n",
      "19/19 [==============================] - 4s 230ms/step - loss: 0.0702\n",
      "31/31 [==============================] - 6s 195ms/step - loss: 0.1633\n",
      "19/19 [==============================] - 4s 230ms/step - loss: 0.0696\n",
      "31/31 [==============================] - 6s 194ms/step - loss: 0.1632\n",
      "19/19 [==============================] - 4s 235ms/step - loss: 0.0695\n",
      "31/31 [==============================] - 6s 200ms/step - loss: 0.1628\n",
      "19/19 [==============================] - 5s 249ms/step - loss: 0.0705\n",
      "31/31 [==============================] - 6s 193ms/step - loss: 0.1634\n",
      "19/19 [==============================] - 4s 228ms/step - loss: 0.0691\n",
      "31/31 [==============================] - 6s 196ms/step - loss: 0.1622\n",
      "19/19 [==============================] - 5s 250ms/step - loss: 0.0692\n",
      "31/31 [==============================] - 6s 194ms/step - loss: 0.1625\n",
      "epochs : 8, batch_size : 256\n",
      "10/10 [==============================] - 4s 388ms/step - loss: 0.0728\n",
      "16/16 [==============================] - 5s 326ms/step - loss: 0.1670\n",
      "10/10 [==============================] - 4s 401ms/step - loss: 0.0742\n",
      "16/16 [==============================] - 5s 324ms/step - loss: 0.1656\n",
      "10/10 [==============================] - 4s 389ms/step - loss: 0.0723\n",
      "16/16 [==============================] - 5s 323ms/step - loss: 0.1640\n",
      "10/10 [==============================] - 4s 390ms/step - loss: 0.0709\n",
      "16/16 [==============================] - 5s 334ms/step - loss: 0.1641\n",
      "10/10 [==============================] - 4s 389ms/step - loss: 0.0712\n",
      "16/16 [==============================] - 5s 324ms/step - loss: 0.1643\n",
      "10/10 [==============================] - 4s 394ms/step - loss: 0.0715\n",
      "16/16 [==============================] - 5s 332ms/step - loss: 0.1653\n",
      "10/10 [==============================] - 4s 404ms/step - loss: 0.0712\n",
      "16/16 [==============================] - 5s 329ms/step - loss: 0.1645\n",
      "10/10 [==============================] - 4s 394ms/step - loss: 0.0714\n",
      "16/16 [==============================] - 5s 326ms/step - loss: 0.1648\n",
      "fold 1: mcrmse 0.2098277338683968 mcrmse pseudo 0.07109895841693822\n",
      "------ fold 2 start -----\n",
      "------ fold 2 start -----\n",
      "------ fold 2 start -----\n",
      "****** load ae model ******\n",
      "epochs : 30, batch_size : 8\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.3922\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.3108\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.1370\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2706\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.1139\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.2533\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.1024\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2442\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0959\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.2369\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0916\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2317\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0884\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2261\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0855\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.2237\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0841\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.2184\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0819\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.2150\n",
      "301/301 [==============================] - 12s 42ms/step - loss: 0.0803\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.2111\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0795\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.2093\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0782\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.2061\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0772\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2036\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0759\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.2015\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0754\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.1987\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0740\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1973\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0746\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.1962\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0731\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1926\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0724\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.1911\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0723\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.1891\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0717\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1884\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0711\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.1867\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0709\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.1843\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0702\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.1833\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0694\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1819\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0695\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.1808\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0681\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.1803\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0684\n",
      "483/483 [==============================] - 18s 38ms/step - loss: 0.1786\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0678\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1771\n",
      "epochs : 10, batch_size : 16\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0674\n",
      "242/242 [==============================] - 12s 49ms/step - loss: 0.1721\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0671\n",
      "242/242 [==============================] - 12s 49ms/step - loss: 0.1719\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0663\n",
      "242/242 [==============================] - 12s 49ms/step - loss: 0.1705\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0675\n",
      "242/242 [==============================] - 12s 49ms/step - loss: 0.1706 1s \n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0664\n",
      "242/242 [==============================] - 12s 49ms/step - loss: 0.1699\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0659\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1691\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0659\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1685\n",
      "151/151 [==============================] - ETA: 0s - loss: 0.065 - 8s 55ms/step - loss: 0.0655\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1686\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0661\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1677\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0656\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1673\n",
      "epochs : 5, batch_size : 32\n",
      "76/76 [==============================] - 6s 82ms/step - loss: 0.0666\n",
      "121/121 [==============================] - 9s 72ms/step - loss: 0.1640\n",
      "76/76 [==============================] - 6s 83ms/step - loss: 0.0642\n",
      "121/121 [==============================] - 9s 71ms/step - loss: 0.1633\n",
      "76/76 [==============================] - 6s 83ms/step - loss: 0.0650\n",
      "121/121 [==============================] - 9s 72ms/step - loss: 0.1631\n",
      "76/76 [==============================] - 6s 82ms/step - loss: 0.0652\n",
      "121/121 [==============================] - 9s 72ms/step - loss: 0.1628\n",
      "76/76 [==============================] - 6s 83ms/step - loss: 0.0647\n",
      "121/121 [==============================] - 9s 72ms/step - loss: 0.1623\n",
      "epochs : 5, batch_size : 64\n",
      "38/38 [==============================] - 5s 134ms/step - loss: 0.0677\n",
      "61/61 [==============================] - 7s 115ms/step - loss: 0.1628\n",
      "38/38 [==============================] - 5s 133ms/step - loss: 0.0667\n",
      "61/61 [==============================] - 7s 112ms/step - loss: 0.1622\n",
      "38/38 [==============================] - 5s 132ms/step - loss: 0.0670\n",
      "61/61 [==============================] - 7s 114ms/step - loss: 0.1620\n",
      "38/38 [==============================] - 5s 134ms/step - loss: 0.0668\n",
      "61/61 [==============================] - 7s 116ms/step - loss: 0.1617\n",
      "38/38 [==============================] - 5s 140ms/step - loss: 0.0667\n",
      "61/61 [==============================] - 7s 116ms/step - loss: 0.1611\n",
      "epochs : 8, batch_size : 128\n",
      "19/19 [==============================] - 4s 236ms/step - loss: 0.0711\n",
      "31/31 [==============================] - 6s 194ms/step - loss: 0.1646\n",
      "19/19 [==============================] - 4s 234ms/step - loss: 0.0709\n",
      "31/31 [==============================] - 6s 195ms/step - loss: 0.1639\n",
      "19/19 [==============================] - 4s 232ms/step - loss: 0.0704\n",
      "31/31 [==============================] - 6s 192ms/step - loss: 0.1638\n",
      "19/19 [==============================] - 4s 230ms/step - loss: 0.0700\n",
      "31/31 [==============================] - 6s 195ms/step - loss: 0.1634\n",
      "19/19 [==============================] - 4s 232ms/step - loss: 0.0697\n",
      "31/31 [==============================] - 6s 195ms/step - loss: 0.1632\n",
      "19/19 [==============================] - 5s 239ms/step - loss: 0.0707\n",
      "31/31 [==============================] - 6s 195ms/step - loss: 0.1629\n",
      "19/19 [==============================] - 4s 229ms/step - loss: 0.0698\n",
      "31/31 [==============================] - 6s 195ms/step - loss: 0.1629\n",
      "19/19 [==============================] - 5s 239ms/step - loss: 0.0694\n",
      "31/31 [==============================] - 6s 199ms/step - loss: 0.1626\n",
      "epochs : 8, batch_size : 256\n",
      "10/10 [==============================] - 4s 386ms/step - loss: 0.0729\n",
      "16/16 [==============================] - 5s 328ms/step - loss: 0.1666\n",
      "10/10 [==============================] - 4s 385ms/step - loss: 0.0752\n",
      "16/16 [==============================] - 5s 327ms/step - loss: 0.1651\n",
      "10/10 [==============================] - 4s 392ms/step - loss: 0.0726\n",
      "16/16 [==============================] - 5s 329ms/step - loss: 0.1645\n",
      "10/10 [==============================] - 4s 400ms/step - loss: 0.0714\n",
      "16/16 [==============================] - 5s 327ms/step - loss: 0.1642\n",
      "10/10 [==============================] - 4s 387ms/step - loss: 0.0721\n",
      "16/16 [==============================] - 5s 334ms/step - loss: 0.1643\n",
      "10/10 [==============================] - 4s 404ms/step - loss: 0.0720\n",
      "16/16 [==============================] - 5s 329ms/step - loss: 0.1642\n",
      "10/10 [==============================] - 4s 390ms/step - loss: 0.0723\n",
      "16/16 [==============================] - 5s 329ms/step - loss: 0.1652\n",
      "10/10 [==============================] - 4s 405ms/step - loss: 0.0737\n",
      "16/16 [==============================] - 5s 327ms/step - loss: 0.1648\n",
      "fold 2: mcrmse 0.2049665298547329 mcrmse pseudo 0.0754384404091124\n",
      "------ fold 3 start -----\n",
      "------ fold 3 start -----\n",
      "------ fold 3 start -----\n",
      "****** load ae model ******\n",
      "epochs : 30, batch_size : 8\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.3649\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.3032\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.1327\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2647\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.1116\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2497\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.1014\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2409\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0950\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2346\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0910\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2282\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0876\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2221\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0851\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2190\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0823\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2144\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0814\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2115\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0796\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2079\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0793\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2054\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0777\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2027\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0761\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2011\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0759\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1984\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0749\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1958\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0741\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1941\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0728\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1927\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0729\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1905\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0710\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1888\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0716\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1871\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0711\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1858\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1835\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0695\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1837\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0699\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1819\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0689\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1798\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0685\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1796\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0680\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1781\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0684\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1764\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0680\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1760\n",
      "epochs : 10, batch_size : 16\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0676\n",
      "242/242 [==============================] - 12s 49ms/step - loss: 0.1715\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0661\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1701\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0659\n",
      "242/242 [==============================] - 12s 49ms/step - loss: 0.1699\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0658\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1694\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0663\n",
      "242/242 [==============================] - 12s 49ms/step - loss: 0.1692\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0653\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1687\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0659\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1673\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0653\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1671\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0651\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1664\n",
      "151/151 [==============================] - 8s 55ms/step - loss: 0.0649\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1656\n",
      "epochs : 5, batch_size : 32\n",
      "76/76 [==============================] - 6s 82ms/step - loss: 0.0669\n",
      "121/121 [==============================] - 9s 72ms/step - loss: 0.1637\n",
      "76/76 [==============================] - 6s 82ms/step - loss: 0.0643\n",
      "121/121 [==============================] - 9s 72ms/step - loss: 0.1622\n",
      "76/76 [==============================] - 6s 83ms/step - loss: 0.0647\n",
      "121/121 [==============================] - 9s 72ms/step - loss: 0.1618\n",
      "76/76 [==============================] - 6s 82ms/step - loss: 0.0647\n",
      "121/121 [==============================] - 9s 71ms/step - loss: 0.1611\n",
      "76/76 [==============================] - 6s 83ms/step - loss: 0.0649\n",
      "121/121 [==============================] - 9s 72ms/step - loss: 0.1608\n",
      "epochs : 5, batch_size : 64\n",
      "38/38 [==============================] - 5s 133ms/step - loss: 0.0666\n",
      "61/61 [==============================] - 7s 113ms/step - loss: 0.1614\n",
      "38/38 [==============================] - 5s 139ms/step - loss: 0.0664\n",
      "61/61 [==============================] - 7s 115ms/step - loss: 0.1602\n",
      "38/38 [==============================] - 5s 132ms/step - loss: 0.0664\n",
      "61/61 [==============================] - 7s 119ms/step - loss: 0.1601\n",
      "38/38 [==============================] - 5s 133ms/step - loss: 0.0669\n",
      "61/61 [==============================] - 7s 116ms/step - loss: 0.1601\n",
      "38/38 [==============================] - 5s 132ms/step - loss: 0.0662\n",
      "61/61 [==============================] - 7s 119ms/step - loss: 0.1599\n",
      "epochs : 8, batch_size : 128\n",
      "19/19 [==============================] - 4s 228ms/step - loss: 0.0705\n",
      "31/31 [==============================] - 6s 196ms/step - loss: 0.1626\n",
      "19/19 [==============================] - 4s 233ms/step - loss: 0.0695\n",
      "31/31 [==============================] - 6s 194ms/step - loss: 0.1625\n",
      "19/19 [==============================] - 4s 229ms/step - loss: 0.0698\n",
      "31/31 [==============================] - 6s 197ms/step - loss: 0.1625 2s \n",
      "19/19 [==============================] - 5s 238ms/step - loss: 0.0693\n",
      "31/31 [==============================] - 6s 195ms/step - loss: 0.1619\n",
      "19/19 [==============================] - 4s 230ms/step - loss: 0.0696\n",
      "31/31 [==============================] - 6s 196ms/step - loss: 0.1617\n",
      "19/19 [==============================] - 5s 240ms/step - loss: 0.0686\n",
      "31/31 [==============================] - 6s 194ms/step - loss: 0.1614\n",
      "19/19 [==============================] - 4s 232ms/step - loss: 0.0694\n",
      "31/31 [==============================] - 6s 194ms/step - loss: 0.1615\n",
      "19/19 [==============================] - 5s 243ms/step - loss: 0.0695\n",
      "31/31 [==============================] - 6s 193ms/step - loss: 0.1609\n",
      "epochs : 8, batch_size : 256\n",
      "10/10 [==============================] - 4s 388ms/step - loss: 0.0735\n",
      "16/16 [==============================] - 5s 324ms/step - loss: 0.1659\n",
      "10/10 [==============================] - 4s 398ms/step - loss: 0.0739\n",
      "16/16 [==============================] - 5s 324ms/step - loss: 0.1641\n",
      "10/10 [==============================] - 4s 405ms/step - loss: 0.0708\n",
      "16/16 [==============================] - 5s 341ms/step - loss: 0.1630\n",
      "10/10 [==============================] - 4s 386ms/step - loss: 0.0707\n",
      "16/16 [==============================] - 5s 331ms/step - loss: 0.1638\n",
      "10/10 [==============================] - 4s 388ms/step - loss: 0.0714\n",
      "16/16 [==============================] - 5s 327ms/step - loss: 0.1633\n",
      "10/10 [==============================] - 4s 390ms/step - loss: 0.0714\n",
      "16/16 [==============================] - 5s 324ms/step - loss: 0.1631\n",
      "10/10 [==============================] - 4s 406ms/step - loss: 0.0700\n",
      "16/16 [==============================] - 5s 329ms/step - loss: 0.1630\n",
      "10/10 [==============================] - 4s 392ms/step - loss: 0.0720\n",
      "16/16 [==============================] - 5s 323ms/step - loss: 0.1644\n",
      "fold 3: mcrmse 0.2144020875978773 mcrmse pseudo 0.07377111090576698\n",
      "------ fold 4 start -----\n",
      "------ fold 4 start -----\n",
      "------ fold 4 start -----\n",
      "****** load ae model ******\n",
      "epochs : 30, batch_size : 8\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.3670\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.3037\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.1331\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.2673\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.1121\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2522\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.1021\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2431\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0964\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2374\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0926\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2310\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0885\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2270\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0862\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2223\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0846\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2193\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0823\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2149\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0804\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2132\n",
      "301/301 [==============================] - 12s 42ms/step - loss: 0.0796\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2098\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0790\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2069\n",
      "301/301 [==============================] - 12s 42ms/step - loss: 0.0775\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2042\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0765\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2038\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0750\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.2005\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0750\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1984\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0737\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1967\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0732\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1950\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0735\n",
      "483/483 [==============================] - 19s 38ms/step - loss: 0.1930\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0720\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1922\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0713\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1904\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0709\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1897\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0713\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1878\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0703\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1864\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0697\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1847\n",
      "301/301 [==============================] - 13s 42ms/step - loss: 0.0691\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1838\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0692\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1832\n",
      "301/301 [==============================] - 12s 42ms/step - loss: 0.0689\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1808\n",
      "301/301 [==============================] - 12s 41ms/step - loss: 0.0682\n",
      "483/483 [==============================] - 19s 39ms/step - loss: 0.1810\n",
      "epochs : 10, batch_size : 16\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0682\n",
      "242/242 [==============================] - 12s 49ms/step - loss: 0.1754\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0674\n",
      "242/242 [==============================] - 12s 49ms/step - loss: 0.1747\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0666\n",
      "242/242 [==============================] - 12s 49ms/step - loss: 0.1737\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0659\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1737\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0671\n",
      "242/242 [==============================] - 12s 49ms/step - loss: 0.1743\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0671\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1722\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0666\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1713\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0661\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1709 0s\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0662\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1698\n",
      "151/151 [==============================] - 8s 54ms/step - loss: 0.0659\n",
      "242/242 [==============================] - 12s 50ms/step - loss: 0.1700\n",
      "epochs : 5, batch_size : 32\n",
      "76/76 [==============================] - 6s 82ms/step - loss: 0.0668\n",
      "121/121 [==============================] - 9s 72ms/step - loss: 0.1672\n",
      "76/76 [==============================] - 6s 82ms/step - loss: 0.0656\n",
      "121/121 [==============================] - 9s 72ms/step - loss: 0.1663\n",
      "76/76 [==============================] - 6s 81ms/step - loss: 0.0675\n",
      "121/121 [==============================] - 9s 72ms/step - loss: 0.1672\n",
      "76/76 [==============================] - 6s 83ms/step - loss: 0.0656\n",
      "121/121 [==============================] - 9s 72ms/step - loss: 0.1653\n",
      "76/76 [==============================] - 6s 81ms/step - loss: 0.0650\n",
      "121/121 [==============================] - 9s 73ms/step - loss: 0.1653\n",
      "epochs : 5, batch_size : 64\n",
      "38/38 [==============================] - 5s 132ms/step - loss: 0.0690\n",
      "61/61 [==============================] - 7s 114ms/step - loss: 0.1646\n",
      "38/38 [==============================] - 5s 136ms/step - loss: 0.0680\n",
      "61/61 [==============================] - 7s 116ms/step - loss: 0.1646\n",
      "38/38 [==============================] - 5s 132ms/step - loss: 0.0676\n",
      "61/61 [==============================] - 7s 118ms/step - loss: 0.1643\n",
      "38/38 [==============================] - 5s 133ms/step - loss: 0.0673\n",
      "61/61 [==============================] - 7s 117ms/step - loss: 0.1640\n",
      "38/38 [==============================] - 5s 132ms/step - loss: 0.0665\n",
      "61/61 [==============================] - 7s 114ms/step - loss: 0.1637\n",
      "epochs : 8, batch_size : 128\n",
      "19/19 [==============================] - 4s 234ms/step - loss: 0.0711\n",
      "31/31 [==============================] - 6s 198ms/step - loss: 0.1665\n",
      "19/19 [==============================] - 4s 229ms/step - loss: 0.0719\n",
      "31/31 [==============================] - 6s 194ms/step - loss: 0.1669\n",
      "19/19 [==============================] - 4s 234ms/step - loss: 0.0709\n",
      "31/31 [==============================] - 6s 197ms/step - loss: 0.1661\n",
      "19/19 [==============================] - 5s 248ms/step - loss: 0.0704\n",
      "31/31 [==============================] - 6s 193ms/step - loss: 0.1656\n",
      "19/19 [==============================] - 5s 252ms/step - loss: 0.0714\n",
      "31/31 [==============================] - 6s 194ms/step - loss: 0.1656 \n",
      "19/19 [==============================] - 4s 236ms/step - loss: 0.0703\n",
      "31/31 [==============================] - 6s 197ms/step - loss: 0.1658\n",
      "19/19 [==============================] - 5s 246ms/step - loss: 0.0704\n",
      "31/31 [==============================] - 6s 196ms/step - loss: 0.1658\n",
      "19/19 [==============================] - 4s 228ms/step - loss: 0.0705\n",
      "31/31 [==============================] - 6s 194ms/step - loss: 0.1655\n",
      "epochs : 8, batch_size : 256\n",
      "10/10 [==============================] - 4s 393ms/step - loss: 0.0752\n",
      "16/16 [==============================] - 5s 326ms/step - loss: 0.1702\n",
      "10/10 [==============================] - 4s 398ms/step - loss: 0.0749\n",
      "16/16 [==============================] - 5s 327ms/step - loss: 0.1695\n",
      "10/10 [==============================] - 4s 388ms/step - loss: 0.0733\n",
      "16/16 [==============================] - 5s 323ms/step - loss: 0.1684\n",
      "10/10 [==============================] - 4s 392ms/step - loss: 0.0730\n",
      "16/16 [==============================] - 5s 334ms/step - loss: 0.1680\n",
      "10/10 [==============================] - 4s 387ms/step - loss: 0.0740\n",
      "16/16 [==============================] - 5s 328ms/step - loss: 0.1682\n",
      "10/10 [==============================] - 4s 390ms/step - loss: 0.0753\n",
      "16/16 [==============================] - 5s 333ms/step - loss: 0.1681\n",
      "10/10 [==============================] - 4s 383ms/step - loss: 0.0752\n",
      "16/16 [==============================] - 5s 337ms/step - loss: 0.1686\n",
      "10/10 [==============================] - 4s 405ms/step - loss: 0.0747\n",
      "16/16 [==============================] - 5s 329ms/step - loss: 0.1682\n",
      "fold 4: mcrmse 0.19943063817603557 mcrmse pseudo 0.08188090880291354\n"
     ]
    }
   ],
   "source": [
    "## here train regression model from pretrain auto encoder model\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "# kfold = KFold(5, shuffle = True, random_state = 42)\n",
    "kfold = GroupKFold(5)\n",
    "\n",
    "scores = []\n",
    "id_list = np.empty([len(X_node)], dtype=object)\n",
    "preds = np.zeros([len(X_node), X_node.shape[1], 5])\n",
    "y_label = np.zeros([len(X_node), X_node.shape[1], 5])\n",
    "\n",
    "id_list_lg = np.empty([len(X_node_lg)], dtype=object)\n",
    "preds_lg = np.zeros([len(X_node_lg), X_node_lg.shape[1], 5])\n",
    "y_label_lg = np.zeros([len(X_node_lg), X_node_lg.shape[1], 5])\n",
    "\n",
    "for i, ((tr_idx, va_idx), ((tr_idx_lg, va_idx_lg))) in enumerate(zip(kfold.split(X_node, As, train['id']), \n",
    "                                         kfold.split(X_node_lg, As_lg, train_lg['id']))):\n",
    "    print(f\"------ fold {i} start -----\")\n",
    "    print(f\"------ fold {i} start -----\")\n",
    "    print(f\"------ fold {i} start -----\")\n",
    "    X_node_tr = X_node[tr_idx]\n",
    "    X_node_va = X_node[va_idx]\n",
    "    As_tr = As[tr_idx]\n",
    "    As_va = As[va_idx]\n",
    "    y_tr = y[tr_idx]\n",
    "    y_va = y[va_idx]\n",
    "    \n",
    "    X_node_tr_lg = X_node_lg[tr_idx_lg]\n",
    "    X_node_va_lg = X_node_lg[va_idx_lg]\n",
    "    As_tr_lg = As_lg[tr_idx_lg]\n",
    "    As_va_lg = As_lg[va_idx_lg]\n",
    "    y_tr_lg = y_lg[tr_idx_lg]\n",
    "    y_va_lg = y_lg[va_idx_lg]\n",
    "    \n",
    "    base = get_base(config)\n",
    "    if ae_epochs > 0:\n",
    "        print(\"****** load ae model ******\")\n",
    "        base.load_weights(\"./base_ae\")\n",
    "    model = get_model(base, config)\n",
    "    if pretrain_dir is not None:\n",
    "        d = f\"./model{i}\"\n",
    "        print(f\"--- load from {d} ---\")\n",
    "        model.load_weights(d)\n",
    "    for epochs, batch_size in zip(epochs_list, batch_size_list):\n",
    "        print(f\"epochs : {epochs}, batch_size : {batch_size}\")\n",
    "        for epoch in range(epochs):\n",
    "            model.fit([X_node_tr_lg, As_tr_lg], [y_tr_lg],\n",
    "                      validation_data=([X_node_va_lg, As_va_lg], [y_va_lg]),\n",
    "                      epochs = 1,\n",
    "                      batch_size = batch_size, validation_freq = 3)\n",
    "            \n",
    "            model.fit([X_node_tr, As_tr], [y_tr],\n",
    "                      validation_data=([X_node_va, As_va], [y_va]),\n",
    "                      epochs = 1,\n",
    "                      batch_size = batch_size, validation_freq = 3)\n",
    "            \n",
    "        \n",
    "#         model.fit([X_node_tr_lg, As_tr_lg], [y_tr_lg],\n",
    "#                   validation_data=([X_node_va_lg, As_va_lg], [y_va_lg]),\n",
    "#                   epochs = epochs,\n",
    "#                   batch_size = batch_size, validation_freq = 3)\n",
    "        \n",
    "#         model.fit([X_node_tr, As_tr], [y_tr],\n",
    "#                   validation_data=([X_node_va, As_va], [y_va]),\n",
    "#                   epochs = epochs,\n",
    "#                   batch_size = batch_size, validation_freq = 3)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    model.save_weights(f\"./model{i}\")\n",
    "    p = model.predict([X_node_va, As_va])\n",
    "    scores.append(mcrmse(y_va, p))\n",
    "    p_lg = model.predict([X_node_va_lg, As_va_lg])\n",
    "    scores.append(mcrmse(y_va_lg, p_lg))\n",
    "    print(f\"fold {i}: mcrmse {scores[-2]} mcrmse pseudo {scores[-1]}\")\n",
    "    \n",
    "    id_list[va_idx] = train.iloc[va_idx][\"id\"].tolist()\n",
    "    preds[va_idx] = p\n",
    "    y_label[va_idx] = y_va\n",
    "    \n",
    "    id_list_lg[va_idx_lg] = train_lg.iloc[va_idx_lg][\"id\"].tolist()\n",
    "    preds_lg[va_idx_lg] = p_lg\n",
    "    y_label_lg[va_idx_lg] = y_va_lg\n",
    "    if one_fold:\n",
    "        break\n",
    "        \n",
    "# pd.to_pickle(preds, \"oof.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20758395855145823, 0.08172962266083396, 0.2098277338683968, 0.07109895841693822, 0.2049665298547329, 0.0754384404091124, 0.2144020875978773, 0.07377111090576698, 0.19943063817603557, 0.08188090880291354]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4821,91,5) (6010,91,5) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f4d10342a1c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmcrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmcrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_lg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_label_lg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-0258d842a5d2>\u001b[0m in \u001b[0;36mmcrmse\u001b[0;34m(t, p)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mseq_len_target\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4821,91,5) (6010,91,5) "
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(mcrmse(preds, y_label))\n",
    "print(mcrmse(preds_lg, y_label_lg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07698997502606883\n"
     ]
    }
   ],
   "source": [
    "# 0.20734392994106435\n",
    "# 0.07698997502606883"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ls = []\n",
    "\n",
    "for i, uid in enumerate(id_list):\n",
    "    single_df = pd.DataFrame(preds[i], columns=targets)\n",
    "    single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n",
    "    \n",
    "    valid_ls.append(single_df)\n",
    "    \n",
    "valid_df = pd.concat(valid_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.to_csv('validation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thinh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "/home/thinh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "p_pub = 0\n",
    "p_pri = 0\n",
    "for i in range(5):\n",
    "    model.load_weights(f\"./model{i}\")\n",
    "    p_pub += model.predict([X_node_pub, As_pub]) / 5\n",
    "    p_pri += model.predict([X_node_pri, As_pri]) / 5\n",
    "    if one_fold:\n",
    "        p_pub *= 5\n",
    "        p_pri *= 5\n",
    "        break\n",
    "\n",
    "for i, target in enumerate(targets):\n",
    "    test_pub[target] = [list(p_pub[k, :, i]) for k in range(p_pub.shape[0])]\n",
    "    test_pri[target] = [list(p_pri[k, :, i]) for k in range(p_pri.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reactivity</th>\n",
       "      <th>deg_Mg_pH10</th>\n",
       "      <th>deg_pH10</th>\n",
       "      <th>deg_Mg_50C</th>\n",
       "      <th>deg_50C</th>\n",
       "      <th>id_seqpos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.767675</td>\n",
       "      <td>0.623866</td>\n",
       "      <td>1.690191</td>\n",
       "      <td>0.528542</td>\n",
       "      <td>0.740489</td>\n",
       "      <td>id_00073f8be_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.272819</td>\n",
       "      <td>3.316724</td>\n",
       "      <td>3.928512</td>\n",
       "      <td>3.225837</td>\n",
       "      <td>2.627201</td>\n",
       "      <td>id_00073f8be_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.681761</td>\n",
       "      <td>0.624205</td>\n",
       "      <td>0.679305</td>\n",
       "      <td>0.742571</td>\n",
       "      <td>0.684655</td>\n",
       "      <td>id_00073f8be_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.296810</td>\n",
       "      <td>1.043877</td>\n",
       "      <td>1.129631</td>\n",
       "      <td>1.630592</td>\n",
       "      <td>1.655349</td>\n",
       "      <td>id_00073f8be_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.756671</td>\n",
       "      <td>0.549650</td>\n",
       "      <td>0.543975</td>\n",
       "      <td>0.858417</td>\n",
       "      <td>0.813196</td>\n",
       "      <td>id_00073f8be_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reactivity  deg_Mg_pH10  deg_pH10  deg_Mg_50C   deg_50C       id_seqpos\n",
       "0    0.767675     0.623866  1.690191    0.528542  0.740489  id_00073f8be_0\n",
       "1    2.272819     3.316724  3.928512    3.225837  2.627201  id_00073f8be_1\n",
       "2    1.681761     0.624205  0.679305    0.742571  0.684655  id_00073f8be_2\n",
       "3    1.296810     1.043877  1.129631    1.630592  1.655349  id_00073f8be_3\n",
       "4    0.756671     0.549650  0.543975    0.858417  0.813196  id_00073f8be_4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_ls = []\n",
    "for df, preds in [(test_pub, p_pub), (test_pri, p_pri)]:\n",
    "    for i, uid in enumerate(df.id):\n",
    "        single_pred = preds[i]\n",
    "\n",
    "        single_df = pd.DataFrame(single_pred, columns=targets)\n",
    "        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n",
    "\n",
    "        preds_ls.append(single_df)\n",
    "\n",
    "preds_df = pd.concat(preds_ls)\n",
    "preds_df.to_csv(\"submission_all.csv\", index = False)\n",
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(457953, 2)\n",
      "(457953, 2)\n",
      "(457953, 2)\n",
      "(457953, 2)\n",
      "(457953, 2)\n"
     ]
    }
   ],
   "source": [
    "sample_df = sub.copy()\n",
    "\n",
    "target_cols = [c for c in sample_df.columns if c != 'id_seqpos']\n",
    "\n",
    "list_id = list(sample_df.id_seqpos.values)\n",
    "output = {}\n",
    "output_df = pd.DataFrame({'id_seqpos': sample_df.id_seqpos.values})\n",
    "\n",
    "for c in target_cols:\n",
    "    output_values = []\n",
    "    x = preds_df.groupby('id_seqpos')[c].mean().reset_index()\n",
    "    print(x.shape)\n",
    "    output_df = pd.merge(output_df, x, on='id_seqpos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(457953, 6)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.to_csv('submission.csv', index=False)\n",
    "output_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20758395855145823, 0.08172962266083396, 0.2098277338683968, 0.07109895841693822, 0.2049665298547329, 0.0754384404091124, 0.2144020875978773, 0.07377111090576698, 0.19943063817603557, 0.08188090880291354]\n",
      "0.14201299892440658\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0.23335501739796655, 0.22334835580793352, 0.23267165843634366, 0.23045706748579886, 0.23826819059727083]\n",
    "# 0.23162005794506269\n",
    "\n",
    "# [0.23220652326610938, 0.2219243451974854, 0.23005732488150737, 0.22844312865044877, 0.23698060044541291]\n",
    "# 0.22992238448819274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
